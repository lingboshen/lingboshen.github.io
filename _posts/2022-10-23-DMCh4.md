---
layout: post
title: Davidson and MacKinnon Chapter 4&mdash;Hypothesis Testing in Linear Regression Model
date: 2022-10-23 11:12:00-0400
description: The fourth chapter of Econometric Theory and Methods
tags: note econometics
categories: 
---
# Some Common Distribution

## The Normal Distrbution


$$\begin{eqnarray}
Z&\sim&N(0,1)\\
X=\mu+\sigma Z&\sim&N(\mu, \sigma)\\
f(x)&=&\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
\end{eqnarray}$$


Any linear combination of independent normally distributed random
variables is normally distributed.

Consider Multivariable Normal Distribution 
$$\begin{eqnarray}
\bm{x}&\sim&N(\bm{\mu}, \bm{\Omega})
\end{eqnarray}$$


If $\bm{a}$ is an $m-$vector of fixed coefficients, then
$\bm{a}^{T}\bm{x}$, which is a linear combination of normal
distribution, follows 
$$\begin{eqnarray}
\bm{a}^{T}\bm{x}&\sim&N(\bm{a}^{T}\bm{\mu}, \bm{a}^{T}\bm{\Omega}\bm{a})
\end{eqnarray}$$


If $\bm{x}$ is any multivariate normal vector with zero covariances, the
components of $\bm{x}$ are mutually independent.

This is a very special property of the multivariate normal distribution.
Usually zero covariance doesn't mean independent.

## The Chi-Squared Distrbution

Suppose the random vector $\bm{z}$ is such that its components
$z_{1}, z_{2},\cdots, z_{m}$ are mutually independent standard normal
distribution random variables, i.e. $\bm{z}\sim N(\bm{0}, \bm{I})$. Then
the random variable $y$ 
$$\begin{eqnarray}
\label{eqd1}
y&\equiv&||\bm{z}||^{2}=\bm{z}^{T}\bm{z}=\sum_{i=1}^{m}z_{i}^{2}
\end{eqnarray}$$


follows the **chi-squared distirbution** with $m$ **degrees of
freedom**. We write is as 
$$\begin{eqnarray}
y&\sim&\chi^{2}(m)
\end{eqnarray}$$


The mean of chi-squared distribution is $m$, and its variance is $2m$.

If $y_{1}\sim \chi^{2}(m_{1})$, $y_{2}\sim \chi^{2}(m_{2})$, and $y_{1}$
and $y_{2}$ are independent, then 
$$\begin{eqnarray}
y&=&y_{1}+y_{2}=\sum_{i=1}^{m_{1}+m_{2}}z_{i}^{2}\sim \chi^{2}(m_{1}+m_{2})
\end{eqnarray}$$


::: proposition
1.  If the $m-$vector $\bm{x}$ is distributed as
    $N(\bm{0},\bm{\Omega})$, then the quadratic form 
$$\begin{eqnarray}
    	\label{eqd2}
    	\bm{x}^{T}\bm{\Omega}^{-1}\bm{x}&\sim&\chi^{2}(m)
    	
    \end{eqnarray}$$


2.  If $\bm{P}$ is a projection matrix with rank $r$ and $\bm{z}$ is an
    $n-$vector that is distributed as $N(\bm{0},\bm{I})$, then the
    quadratic form 
$$\begin{eqnarray}
    	\label{eqd3}
    	\bm{z}^{T}\bm{P}\bm{z}&\sim&\chi^{2}(r)
    	
    \end{eqnarray}$$

:::

::: proof
*Proof.*

1.  Let $\bm{z}=\bm{A}^{-1}\bm{x}$, where $\bm{A}\bm{A}^{T}=\Omega$

    Since the vector $\bm{x}$ is multivariate normal with mean vector
    $\bm{0}$, so is the vector $\bm{A}^{-1}\bm{x}$. The covariance of
    $\bm{A}^{-1}\bm{x}$ is 
$$\begin{eqnarray}
    E\left(\bm{A}^{-1}\bm{x}\bm{x}^{T}(\bm{A}^{T})^{-1}\right)&=&\bm{A}^{-1}E\left(\bm{x}\bm{x}^{T}\right)(\bm{A}^{T})^{-1}\\
    &=&\bm{A}^{-1}\bm{\Omega}(\bm{A}^{T})^{-1}=\bm{A}^{-1}\bm{A}\bm{A}^{T}(\bm{A}^{T})^{-1}\\
    &=&\bm{I}_{m}
    \end{eqnarray}$$
 So $\bm{z}=\bm{A}^{-1}\bm{x}\sim N(\bm{0}, \bm{I})$
    Considering the quadratic form $\bm{x}^{T}\bm{\Omega}^{-1}\bm{x}$,
    we have 
$$\begin{eqnarray}
    \bm{x}^{T}\bm{\Omega}^{-1}\bm{x}&=&\bm{x}^{T}(\bm{A}\bm{A}^{T})^{-1}\bm{x}\\
    &=&\bm{x}^{T}(\bm{A}^{T})^{-1}\bm{A}^{-1}\bm{x}\\
    &=&\bm{z}^{T}\bm{z}\sim \chi^{2}(m)
    \end{eqnarray}$$


2.  Suppose $\bm{P}$ projects on to the span of the columns of an
    $n\times r$ matrix $\bm{Z}$. This allows us to write
    
$$\begin{eqnarray}
    \bm{z}^{T}\bm{P}\bm{z}&=&\bm{z}^{T}\bm{Z(Z^{T}Z)^{-1}Z^{T}}\bm{z}
    \end{eqnarray}$$
 Let $\bm{x}=\bm{Z}^{T}\bm{z}$, and
    $\bm{x}\sim N(\bm{0},\bm{Z}^{T}\bm{Z})$. Therefore,
    
$$\begin{eqnarray}
    \bm{z}^{T}\bm{P}\bm{z}&=&\bm{x}^{T}(\bm{Z}^{T}\bm{Z})^{-1}\bm{x}
    \end{eqnarray}$$
 where $\bm{Z}^{T}\bm{Z}$ is the variance of
    $\bm{x}$. Use the first part of the theorem, we prove that
    $\bm{z}^{T}\bm{P}\bm{z}$ is distributed as $\chi^{2}(r)$.

 â—»
:::

## The Student's t Distribution

If $z\sim N(0,1)$ and $y\sim \chi^{2}(m)$, and $z$ and $y$ are
independent, then the random variable 
$$\begin{eqnarray}
\label{eqd4}
z&\equiv&\frac{z}{(y/m)^{1/2}}
\end{eqnarray}$$
 is said to follow the **Student's t disribution** with
$m$ degrees of freedom. We write is as 
$$\begin{eqnarray}
t&\sim&t(m)
\end{eqnarray}$$


## The F Distribution

If $y_{1}\sim \chi^{2}(m_{1})$ and $y_{2}\sim \chi^{2}(m_{2})$, and
$y_{1}$ and $y_{2}$ are independent, then the random variable

$$\begin{eqnarray}
\label{eqd5}
F&\equiv&\frac{y_{1}/m_{1}}{y_{2}/m_{2}}
\end{eqnarray}$$
 is said to follow the **F distribution** with $m_{1}$
and $m_{2}$ degrees of freedom. We write it as 
$$\begin{eqnarray}
F&\sim&F(m_{1}, m_{2})
\end{eqnarray}$$


# Tests of a Single Restriction

We want to test $\beta_{2}=0$ 
$$\begin{eqnarray}
\label{eq6}
\bm{y}&=&\bm{X}_{1}\bm{\beta}_{1}+\beta_{2}x_{2}+\bm{u}\\
\nonumber
\bm{M}_{1}\bm{y}&=&\beta_{2}\bm{M}_{1}x_{2}+\bm{M}_{1}\bm{u}
\end{eqnarray}$$


We find that 
$$\begin{eqnarray}
\hat{\beta}_{2}&=&\frac{\bm{x}_{2}^{T}\bm{M}_{1}\bm{y}}{\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}}\\
Var(\hat{\beta}_{2})&=&\sigma^{2}\left(\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{-1}
\end{eqnarray}$$


For the null hypothesis that $\beta_{2}=0$, this yields a test statistic

$$\begin{eqnarray}
\label{eq7}
z_{\beta_{2}}&=&\frac{\bm{x}_{2}^{T}\bm{M}_{1}\bm{y}}{\sigma\left(\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{1/2}}\\
\label{eq8}
&=&\frac{\bm{x}_{2}^{T}\bm{M}_{1}\bm{u}}{\sigma\left(\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{1/2}}\sim N(0,1)
\end{eqnarray}$$


However, we do not know $\sigma$. We need to replace $\sigma$ by $s$

$$\begin{eqnarray}
s^{2}&=&\frac{\bm{u}^{T}\bm{u}}{n-k}=\frac{(\bm{M_{X}y})^{T}\bm{M_{X}y}}{n-k}\\
&=&\frac{\bm{y}^{T}\bm{M_{X}y}}{n-k}
\end{eqnarray}$$


and we obtain the test statistic 
$$\begin{eqnarray}
\nonumber
t_{\beta_{2}}&=&\frac{\bm{x}_{2}^{T}\bm{M}_{1}\bm{y}}{s\left(\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{1/2}}\\
\nonumber
&=&\frac{\bm{x}_{2}^{T}\bm{M}_{1}\bm{y}}{\frac{s}{\sigma}\sigma\left(\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{1/2}}\\
\nonumber
&=&\frac{z_{\beta_{2}}}{s/\sigma}\\
\label{eq9}
&=&\frac{z_{\beta_{2}}}{\sqrt{\frac{\bm{y}^{T}\bm{M_{X}y}}{\sigma^{2}}/n-k}}
\end{eqnarray}$$


Then we need to show that
$\frac{\bm{y}^{T}\bm{M_{X}y}}{\sigma^{2}}\sim \chi^{2}(n-k)$. If so,
([\[eq9\]](#eq9){reference-type="ref" reference="eq9"}) is $t$
distribution with degree of freedom $(n-k)$. 
$$\begin{eqnarray}
\label{eq10}
\frac{\bm{y}^{T}\bm{M_{X}y}}{\sigma^{2}}&=&\frac{\bm{u}^{T}\bm{M_{X}u}}{\sigma^{2}}=\bm{\varepsilon}^{T}\bm{M_{X}\varepsilon}
\end{eqnarray}$$
 where
$\bm{\varepsilon}\equiv\bm{u}/\sigma\sim N(\bm{0}, \bm{1})$. The second
part of Theorem 1 tells us that rightmost expression in the
([\[eq10\]](#eq10){reference-type="ref" reference="eq10"}) is
distributed as $\chi^{2}(n-k)$

# Tests of Several Restrictions

Suppose that there are $r$ restrictions, with $r\leq k$.

$$\begin{eqnarray}
\label{eq11}
H_{0}&:& \bm{y}=\bm{X}_{1}\bm{\beta}_{1}+\bm{u}\\
\label{eq12}
H_{1}&:& \bm{y}=\bm{X}_{1}\bm{\beta}_{1}+\bm{X}_{2}\bm{\beta}_{2}+\bm{u}
\end{eqnarray}$$
 where $\bm{X}_{1}$ is an $n\times k_{1}$ matrix,
$\bm{X}_{2}$ is an $n\times k_{2}$ matrix, $\bm{\beta}_{1}$ is a
$k_{1}$-vector, $\bm{\beta}_{2}$ is a $k_{2}$-vector, $k=k_{1}+k_{2}$,
and the number of restrictions $r=k_{2}$.

The test statistic is 
$$\begin{eqnarray}
\label{eq13}
F_{\beta_{2}}&\equiv&\frac{(RSSR-USSR)/r}{USSR/(n-k)}
\end{eqnarray}$$
 Under the null hypothesis, this test statistic follow
the $F$ distribution with $r$ and $n-k$ degrees of freedom.

It is easy to find that 
$$\begin{eqnarray}
RSSR&=&(\bm{M}_{1}\bm{y})^{T}\bm{M}_{1}\bm{y}=\bm{y}^{T}\bm{M}_{1}\bm{y}\\
USSR&=&(\bm{M_{X}}\bm{y})^{T}\bm{M_{X}}\bm{y}=\bm{y}^{T}\bm{M_{X}}\bm{y}
\end{eqnarray}$$


By FWL theorem, the $USSR$ is the SSR from the FWL regression

$$\begin{eqnarray}
\label{eq14}
\bm{M}_{1}\bm{y}&=&\bm{M}_{1}\bm{X}_{2}\bm{\beta}_{2}+\bm{M}_{1}\bm{u}
\end{eqnarray}$$


and the $USSR$ becomes 
$$\begin{eqnarray}
\nonumber
USSR&=&TSS-ESS\\\nonumber
&=&(\bm{M}_{1}\bm{y})^{T}\bm{M}_{1}\bm{y}\\\nonumber
&-&[\bm{M}_{1}\bm{X}_{2}((\bm{M}_{1}\bm{X}_{2})^{T}\bm{M}_{1}\bm{X}_{2})^{-1}(\bm{M}_{1}\bm{X}_{2})^{T}\bm{M}_{1}\bm{y}]^{T}\bm{M}_{1}\bm{X}_{2}((\bm{M}_{1}\bm{X}_{2})^{T}\bm{M}_{1}\bm{X}_{2})^{-1}(\bm{M}_{1}\bm{X}_{2})^{T}\bm{M}_{1}\bm{y}\\\nonumber
&=&\bm{y}^{T}\bm{M}_{1}\bm{y}-[\bm{M}_{1}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}]^{T}\bm{M}_{1}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}\\\nonumber
&=&\bm{y}^{T}\bm{M}_{1}\bm{y}-\bm{y}^{T}\bm{M_{1}^{T}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}^{T}\bm{M}_{1}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}\\\nonumber
&=&\bm{y}^{T}\bm{M}_{1}\bm{y}-\bm{y}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}\\
\label{eq15}
&=&\bm{y}^{T}\bm{M}_{1}\bm{y}-\bm{y}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}
\end{eqnarray}$$


Therefore, 
$$\begin{eqnarray}
RSSR-USSR&=&\bm{y}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}
\end{eqnarray}$$


Now the $F$ statistics ([\[eq14\]](#eq14){reference-type="ref"
reference="eq14"}) can be written as 
$$\begin{eqnarray}
\label{eq16}
F_{\beta_{2}}&=&\frac{\bm{y}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}/r}{\bm{y}^{T}\bm{M_{X}}\bm{y}/(n-k)}
\end{eqnarray}$$


Under the null hypothesis $\bm{y}=\bm{X}_{1}\bm{\beta}_{1}+\bm{u}$

$$\begin{eqnarray}
\bm{M_{X}y}&=&\bm{M_{X}u}\\
\bm{M}_{1}\bm{y}&=&\bm{M}_{1}\bm{u}\\
\end{eqnarray}$$


Thus, under this hypothesis, the $F$ statistics
([\[eq16\]](#eq16){reference-type="ref" reference="eq16"}) becomes to

$$\begin{eqnarray}
\nonumber
F_{\beta_{2}}&=&\frac{\bm{u}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{u}/r}{\bm{u}^{T}\bm{M_{X}}\bm{u}/(n-k)}\\
\label{eq17}
&=&\frac{\bm{\varepsilon}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{\varepsilon}/r}{\bm{\varepsilon}^{T}\bm{M_{X}}\bm{\varepsilon}/(n-k)}
\end{eqnarray}$$
 where $\bm{\varepsilon}=\bm{u}/\sigma$

The denominator of ([\[eq17\]](#eq17){reference-type="ref"
reference="eq17"}) is distributed as $\chi^{2}(n-k)$. The quadratic form
of numerator can be written as
$\bm{\varepsilon}^{T}\bm{P}_{\bm{M}_{1}\bm{X}_{2}}\bm{\varepsilon}$, it
is distributed as $\chi^{2}(r)$.

## Chow Test

It is natural to divide a sample into two subsamples, e.g. larger/small
firms, men/women. We want to test whether a linear regression model has
the same coefficients for both the subsamples. Chow test can solve this
problem.

Suppose there are two subsamples, of lengths $n_{1}$ and $n_{2}$, with
$n=n_{1}+n_{2}$. Both $n_{1}$ and $n_{2}$ are greater than $k$, the
number of regressors. 
$$\begin{eqnarray}
\y=\left[\begin{array}{c}
\y_{1}\\
\y_{2}
\end{array}\right]
&\mbox{and}&
\x=\left[\begin{array}{c}
\x_{1}\\
\x_{2}
\end{array}\right]
\end{eqnarray}$$
 Even we need different parameter vectors $\be_{1}$ and
$\be_{2}$ for two subsamples, we can nonetheless put the subsamples
together into a regression model 
$$\begin{eqnarray}
\label{eq25}
\left[\begin{array}{c}
\y_{1}\\
\y_{2}
\end{array}\right]=
\left[\begin{array}{c}
\x_{1}\\
\x_{2}
\end{array}\right]\be_{1}+
\left[\begin{array}{c}
\bm{0}\\
\x_{2}
\end{array}\right]\bm{\gamma}+\bu
&,&\bu\sim N(\bm{0},\sigma^{2}\bm{I})
\end{eqnarray}$$
 where $\be_{1}+\bm{\gamma}=\be_{2}$.

We could rewrite ([\[eq25\]](#eq25){reference-type="ref"
reference="eq25"}) 
$$\begin{eqnarray}
\label{eq26}
\y=\x\be_{1}+\bm{Z}\bm{\gamma}+\bu&,&\bu\sim N(\bm{0},\sigma^{2}\bm{I})
\end{eqnarray}$$
 The null hypothesis is $H_{0}:\bm{\gamma}=\bm{0}$ and it
has been expressed as a set of $k$ zero restrictions. We can use classic
$F$ test. However, if $SSR_{1}$ and $SSR_{2}$ denote the sums of squared
residuals from two regressions, and $RSSR$ denotes the sum of squared
residuals from regressing $\y$ on $\x$, the $F$ statistic becomes

$$\begin{eqnarray}
\label{eq27}
F_{\bm{\gamma}}&=&\frac{(RSSR-SSR_{1}-SSR_{2})/k}{(SSR_{1}+SSR_{2})/(n-2k)}
\end{eqnarray}$$


# Large-Sample Tests in Linear Regression Models

Asymptotic theory is concerned with the distributions of estimators and
test statistics as the sample size $n$ tends to infinity.

## Laws of Large Number

A law of large numbers may apply to any quantity which can be written as
an average of $n$ random variable, that is, $1/n$ times their sum.

$$\begin{eqnarray}
\bar{x}&\equiv&\frac{1}{n}\sum_{t=1}^{n}x_{t}
\end{eqnarray}$$
 where $x_{t}$ are **independent** random variables, each
with **bounded finite variance** $\sigma_{t}^{2}$ and **with a common
mean** $\mu$. As $n\to \infty$, $\bar{x}\to\mu$.

There are many different LLNs, some of which do not require that the
individual random variables have a common mean or be independent,
although the amount of dependence must be limited.

If we can apply a LLN to any random average, we can treat it as a
nonrandom quantity for the purpose of asymptotic analysis.

## Central Limit Theorems

In many circumstance, $1/\sqrt{n}$ times the sum of $n$ centered random
variables will approximately follow a normal distribution.

Suppose that the random variables $x_{t}$, $t=1,\dots,n$ are
independently and identically distributed with mean $\mu$ and variance
$\sigma^{2}$. Then according to the Lindebery-LÃ©vy central limit theorem

$$\begin{eqnarray}
z&\equiv&\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\frac{x_{t}-\mu}{\sigma}
\end{eqnarray}$$
 is **asymptotically distributed** as $N(0,1)$. This
means that as $n\to\infty$, the random variable $z$ tends to a random
variable which follows the $N(0,1)$.

For a sequence of random variables, $x_{t}$, $t=1,\dots,n$ with
$E(x_{t})=0$ 
$$\begin{eqnarray}
\plim_{n\to\infty}n^{-1/2}\sum_{t=1}^{n}x_{t}=x_{0}\sim N\left(0,\lim_{n\to\infty} \frac{1}{n}\sum_{t=1}^{n}Var(x_{t})\right)
\end{eqnarray}$$


It can also be applied to the multivariate version of CLTs.

## Asymptotic Tests

Suppose that the DGP is 
$$\begin{eqnarray}
\label{eq18}
\bm{y}&=&\bm{X\beta}_{0}+\bm{u}\\
\nonumber
\bm{u}&\sim&IID(\bm{0}, \sigma^{2}_{0}\bm{I})
\end{eqnarray}$$
 We make another assumptions 
$$\begin{eqnarray}
\label{eq19}
E(u_{t}|\bm{X}_{t})&=&0\\
\nonumber
E(u_{t}^{2}|\bm{X}_{t})&=&\sigma^{2}_{0}
\end{eqnarray}$$
 From the point of view of the error terms, it says that
they are **innovations**. From the point of view of the explanatory
variables $\bm{X}_{t}$, they are **predetermined** with respect to the
errors terms.

To be able to use asymptotic results, we assume that the DGP for the
explanatory variables is such that 
$$\begin{eqnarray}
\label{eq20}
\plim_{n\to\infty}\frac{1}{n}\bm{X}^{T}\bm{X}&=&\bm{S}_{\bm{X}^{T}\bm{X}}
\end{eqnarray}$$
 where $\bm{S}_{\bm{X}^{T}\bm{X}}$ is a finite,
deterministic, positive definite matrix.

We rewrite $t_{\beta_{2}}$ as 
$$\begin{eqnarray}
\label{eq21}
t_{\beta_{2}}&=&\left(\frac{\bm{y}^{T}\bm{M_{X}\bm{y}}}{n-k}\right)^{-1/2}\frac{n^{-1/2}\bm{x}_{2}^{T}\bm{M}_{1}\bm{y}}{\left(n^{-1}\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{1/2}}
\end{eqnarray}$$


As $n\to\infty$, $s^{2}\equiv\frac{\bm{y}^{T}\bm{M_{X}\bm{y}}}{n-k}$
tends to $\sigma_{0}^{2}$. So the first factor in
([\[eq21\]](#eq21){reference-type="ref" reference="eq21"}) tends to
$1/\sigma_{0}$ as $n\to\infty$.

When DGP with $\beta_{2}=0$, we have that
$\bm{M}_{1}\bm{y}=\bm{M}_{1}\bm{u}$, and so
([\[eq21\]](#eq21){reference-type="ref" reference="eq21"}) is
asymptotically equivalent to 
$$\begin{eqnarray}
\label{eq22}
t_{\beta_{2}}&=&\frac{n^{-1/2}\bm{x}_{2}^{T}\bm{M}_{1}\bm{u}}{\sigma_{0}\left(n^{-1}\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}\right)^{1/2}}
\end{eqnarray}$$


If we reinstate the assumption that the regressors are exogenous, the
conditional variance of the numerator of
([\[eq22\]](#eq22){reference-type="ref" reference="eq22"}) is

$$\begin{eqnarray}
E(\bm{x}_{2}^{T}\bm{M}_{1}\bm{u}\bm{u}^{T}\bm{M}_{1}\bm{x}_{2}|\bm{X})&=&\sigma_{0}^{2}\bm{x}_{2}^{T}\bm{M}_{1}\bm{x}_{2}
\end{eqnarray}$$


Thus ([\[eq22\]](#eq22){reference-type="ref" reference="eq22"}) has mean
$0$ and variance $1$, conditional on $\bm{X}$. They are also the
unconditional mean and variance.

Under the null hypothesis, with exogenous regressors, 
$$\begin{eqnarray}
\label{eq23}
t_{\beta_{2}}&\sim&N(0,1)
\end{eqnarray}$$


## The $t$ Test with Predetermined Regressors

To the $k-$vector 
$$\begin{eqnarray}
\bm{v}&\equiv&n^{-1/2}\bm{X}^{T}\bm{u}=n^{-1/2}\sum_{t=1}^{n}u_{t}\bm{X}^{T}
\end{eqnarray}$$


We assume $E(u_{t}|\bm{X}_{t})=0$. This implies that
$E(u_{t}\bm{X}_{t}^{T})=0$, as required for the CLT, which then tells us
that 
$$\begin{eqnarray}
\bm{v}&\sim&N\left(\bm{0}, \lim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n}Var(u_{t}\bm{X}_{t}^{T})\right)=N\left(\bm{0}, \lim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n}E(u_{t}^{2}\bm{X}_{t}^{T}\bm{X}_{t})\right)
\end{eqnarray}$$



$$\begin{eqnarray}
\lim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n}E(u_{t}^{2}\bm{X}_{t}^{T}\bm{X}_{t})&=&\lim_{n\to\infty}\sigma_{0}^{2}\frac{1}{n}\sum_{t=1}^{n}E(\bm{X}_{t}^{T}\bm{X}_{t})\\
&=&\sigma_{0}^{2}\plim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n}\bm{X}_{t}^{T}\bm{X}_{t}\\
&=&\sigma_{0}^{2}\plim_{n\to\infty}\frac{1}{n}\bm{X}^{T}\bm{X}\\
&=&\sigma_{0}^{2}\bm{S}_{\bm{X}^{T}\bm{X}}
\end{eqnarray}$$


## Asymptotic $F$ Tests

$F$ statistics ([\[eq16\]](#eq16){reference-type="ref"
reference="eq16"}) under the null hypothesis that $\bm{\beta}_{2}=0$ can
be rewritten as 
$$\begin{eqnarray}
\nonumber
F_{\beta_{2}}&=&\frac{\bm{y}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{y}/r}{\bm{y}^{T}\bm{M_{X}}\bm{y}/(n-k)}\\
\nonumber
&=&\frac{\bm{\varepsilon}^{T}\bm{M_{1}}\bm{X}_{2}(\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{\varepsilon}/r}{\bm{\varepsilon}^{T}\bm{M_{X}}\bm{\varepsilon}/(n-k)}\\
\label{eq24}
&=&\frac{n^{-1/2}\bm{\varepsilon}^{T}\bm{M_{1}}\bm{X}_{2}(n^{-1}\bm{X}_{2}^{T}\bm{M}_{1}\bm{X}_{2})^{-1}n^{-1/2}\bm{X}_{2}^{T}\bm{M}_{1}\bm{\varepsilon}/r}{\bm{\varepsilon}^{T}\bm{M_{X}}\bm{\varepsilon}/(n-k)}
\end{eqnarray}$$
 where $\bm{\varepsilon}=\bm{u}/\sigma_{0}$.
