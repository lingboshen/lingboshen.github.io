---
layout: post
title: Davidson and MacKinnon Chapter 7&mdash;Generalized Least Squares and Related Topics
date: 2022-10-23 11:12:00-0400
description: The seventh chapter of Econometric Theory and Methods
tags: note econometics
categories: 
---
 
\section{Introduction}
We will be concerned with the model
\begin{eqnarray}
\label{eq7.1}
\bm{y}&=&\bm{X\beta}+\bm{u}\ \ ,\ \ E(\bm{uu}^{T})=\bm{\Omega}
\end{eqnarray}
where  $$\bm{\Omega}$$ is the covariance matrix of the error terms, is a positive definite $$n\times n$$ matrix. We consider following cases for $$\bm{\Omega}$$:
\begin{enumerate}
\item $$\bm{\Omega}=\sigma^{2}\bm{I}$$: (\ref{eq7.1}) is the classic linear regression model
\item $$\bm{\Omega}$$ is diagonal with nonconstant diagonal elements: heteroskedastic
\item $$\bm{\Omega}$$ is not diagonal
\end{enumerate}

\section{The GLS Estimator}
We transform the model (\ref{eq7.1}) so that the transformed model satisfied the conditions of the Gauss-Markov theorem. Let $$\bm{\Psi}$$ be a $$n\times n$$ triangular matrix that satisfies 
\begin{eqnarray}
\label{eq7.2}
\bm{\Omega}^{-1}&=&\bm{\Psi}\bm{\Psi}^{T}
\end{eqnarray}

Premultiplying (\ref{eq7.1}) by $$\bm{\Psi}^{T}$$ gives
\begin{eqnarray}
\label{eq7.3}
\bm{\Psi}^{T}\bm{y}&=&\bm{\Psi}^{T}\bm{X\beta}+\bm{\Psi}^{T}\bm{u}
\end{eqnarray}

The OLS estimator of $$\bm{\beta}$$ from regression (\ref{eq7.3}) is
\begin{eqnarray}
\label{eq7.4}
\hat{\bm{\beta}}_{GLS}&=&\left(\bm{X}^{T}\bm{\Psi}\bm{\Psi}^{T}\bm{X}\right)^{-1}\bm{X}^{T}\bm{\Psi}\bm{\Psi}^{T}\bm{y}=\left(\bm{X}^{T}\bm{\Omega}^{-1}\bm{X}\right)^{-1}\bm{X}^{T}\bm{\Omega}^{-1}\bm{y}
\end{eqnarray}
This estimator is called the \textbf{generalized least squares}, or \textbf{GLS}, estimator of $$\bm{\beta}$$

It is easy to show that the covariance matrix of transformed model (\ref{eq7.3}) is the identity matrix
\begin{eqnarray*}
E(\bm{\Psi}^{T}\bm{u}\bm{u}^{T}\bm{\Psi})&=&\bm{\Psi}^{T}E(\bm{u}\bm{u}^{T})\bm{\Psi}\\
&=&\bm{\Psi}^{T}\bm{\Omega}\bm{\Psi}\\
&=&\bm{\Psi}^{T}(\bm{\Psi}\bm{\Psi}^{T})^{-1}\bm{\Psi}\\
&=&\bm{\Psi}^{T}(\bm{\Psi}^{T})^{-1}\bm{\Psi}^{-1}\bm{\Psi}=\bm{I}\\
\end{eqnarray*}

And we find
\begin{eqnarray}
\label{eq7.5}
Var(\hat{\bm{\beta}}_{GLS})&=&(\bm{X}^{T}\bm{\Psi}\bm{\Psi}^{T}\bm{X})^{-1}=(\bm{X}^{T}\bm{\Omega}^{-1}\bm{X})^{-1}
\end{eqnarray}

The generalized least squares estimator $$\hat{\bm{\beta}}_{GLS}$$ can also be obtained by minimizing the \textbf{GLS criterion function}
\begin{eqnarray}
\nonumber
(\bm{\Psi}^{T}\bm{u})^{T}\bm{\Psi}^{T}\bm{u}&=&(\bm{\Psi}^{T}\bm{y}-\bm{\Psi}^{T}\bm{X\beta})^{T}(\bm{\Psi}^{T}\bm{y}-\bm{\Psi}^{T}\bm{X\beta})\\
\nonumber
&=&(\bm{y}-\bm{X\beta})^{T}\bm{\Psi}\bm{\Psi}^{T}(\bm{y}-\bm{X\beta})\\
\label{eq7.6}
&=&(\bm{y}-\bm{X\beta})^{T}\bm{\Omega}^{-1}(\bm{y}-\bm{X\beta})
\end{eqnarray}

\subsection{Efficiency of the GLS Estimator}
The GLS estimator $$\hat{\bm{\beta}}_{GLS}$$ defined in (\ref{eq7.4}) is also the solution of the set of moment conditions 
\begin{eqnarray}
\nonumber
(\bm{\Psi}^{T}\bm{X})^{T}(\bm{\Psi}^{T}\bm{y}-\bm{\Psi}^{T}\bm{X\beta})&=&\bm{0}\\
\nonumber
\bm{X}^{T}\bm{\Psi}\bm{\Psi}^{T}(\bm{y}-\bm{X\beta})&=&\bm{0}\\
\label{eq7.7}
\bm{X}^{T}\bm{\Omega}^{-1}(\bm{y}-\bm{X\beta}_{GLS})&=&\bm{0}
\end{eqnarray}

GLS estimator is a method of moments estimator. A general MM estimator for the linear regression model (\ref{eq7.1}) is defined in terms of an $$n\times k$$ matrix of exogenous variables $$\bm{W}$$
\begin{eqnarray}
\label{eq7.8}
\bm{W}^{T}(\bm{y}-\bm{X}\bm{\beta})&=&\bm{0}
\end{eqnarray}

We obtain the MM estimator 
\begin{eqnarray}
\label{eq7.9}
\hat{\bm{\beta}}_{\bm{W}}&\equiv&(\bm{W}^{T}\bm{X})^{-1}\bm{W}^{T}\bm{y}
\end{eqnarray}

The GLS estimator (\ref{eq7.4}) is a special case of the MM estimator, with $$\bm{W}=\bm{\Omega}^{-1}\bm{X}$$.

We can rewrite $$\hat{\bm{\beta}}_{\bm{W}}=\bm{\beta}_{0}+(\bm{W}^{T}\bm{X})^{-1}\bm{W}^{T}\bm{u}$$. Thus, the covariance matrix of $$\hat{\bm{\beta}}_{\bm{W}}$$ is
\begin{eqnarray}
\nonumber
Var(\hat{\bm{\beta}}_{\bm{W}})&=&E\left((\hat{\bm{\beta}}_{\bm{W}}-\bm{\beta}_{0})(\hat{\bm{\beta}}_{\bm{W}}-\bm{\beta}_{0})^{T}\right)\\
\nonumber
&=&E\left((\bm{W}^{T}\bm{X})^{-1}\bm{W}^{T}\bm{u}\bm{u}^{T}\bm{W}(\bm{X}^{T}\bm{W})^{-1}\right)\\
\label{eq7.10}
&=&(\bm{W}^{T}\bm{X})^{-1}\bm{W}^{T}\bm{\Omega}\bm{W}(\bm{X}^{T}\bm{W})^{-1}
\end{eqnarray}

The efficiency of the GLS estimator can be verified by showing that the difference between (\ref{eq7.10}), the covariance matrix of the MM estimator $$\hat{\bm{\beta}}_{\bm{W}}$$, and (\ref{eq7.5}), the covariance matrix of GLS estimator, $$\hat{\bm{\beta}}_{GLS}$$, is a positive semidefinite matrix. That is, the matrix
\begin{eqnarray}
\label{eq7.11}
(\bm{W}^{T}\bm{X})^{-1}\bm{W}^{T}\bm{\Omega}\bm{W}(\bm{X}^{T}\bm{W})^{-1}
&-&(\bm{X}^{T}\bm{\Omega}^{-1}\bm{X})^{-1}
\end{eqnarray}
is a positive semidefinite matrix.

By the theory of linear algebra, (\ref{eq7.11}) is positive semidefinite if and only if 
\begin{eqnarray*}
\left((\bm{X}^{T}\bm{\Omega}^{-1}\bm{X})^{-1}\right)^{-1}&-&\left((\bm{W}^{T}\bm{X})^{-1}\bm{W}^{T}\bm{\Omega}\bm{W}(\bm{X}^{T}\bm{W})^{-1}\right)^{-1}\\
\bm{X}^{T}\bm{\Omega}^{-1}\bm{X}&-&\bm{X}^{T}\bm{W}(\bm{W}^{T}\bm{\Omega}\bm{W})^{-1}\bm{W}^{T}\bm{X}\\
\end{eqnarray*}
is positive semidefinite.

The GLS estimator $$\hat{\bm{\beta}}_{GLS}$$ is typically more efficient than the more general MM estimator $$\hat{\bm{\beta}}_{W}$$. Because the OLS estimator $$\hat{\bm{\beta}}_{OLS}$$ is just special case of $$\hat{\bm{\beta}}_{W}$$ when $$\bm{W}=\bm{X}$$, we could conclude that $$\hat{\bm{\beta}}_{GLS}$$ will in most cases be more efficient, and will never be less efficient, than the OLS estimator $$\hat{\bm{\beta}}_{OLS}$$.

\section{Computing GLS Estimates}
Suppose $$\bm{\Omega}=\sigma^{2}\bm{\Delta}$$, and $$\bm{\Delta}$$ is known and $$\sigma^{2}$$ is unknown. If we replace $$\bm{\Omega}$$ by $$\bm{\Delta}$$ in (\ref{eq7.2}) of $$\bm{\Psi}$$,
\begin{eqnarray*}
\bm{\Delta}^{-1}&=&\bm{\Psi}\bm{\Psi}^{T}
\end{eqnarray*}
we still run regression (\ref{7.3})
\begin{eqnarray*}
\bm{\Psi}^{T}\bm{y}&=&\bm{\Psi}^{T}\bm{X\beta}+\bm{\Psi}^{T}\bm{u}
\end{eqnarray*}
The GLS estimates will be the same whether we use $$\bm{\Omega}$$ or $$\bm{\Delta}$$. 

However, if $$\sigma^{2}$$ is known, we can use the true covariance matrix (\ref{eq7.5}). Otherwise, we must estimate covariance matrix
\begin{eqnarray}
\label{eq7.12}
\widehat{Var}(\hat{\bm{\beta}}_{GLS})&=&s^{2}(\bm{X}^{T}\bm{\Delta}^{-1}\bm{X})^{-1}
\end{eqnarray}

\subsection{Weighted Least Squares}
GLS estimation will be easy to do if the matrix $$\bm{\Psi}$$ is known and allow us to calculate $$\bm{\Psi}^{T}\bm{x}$$.

When error terms are heteroskedastic but uncorrelated, i.e., $$\bm{\Omega}$$ is diagonal. Let $$\omega_{t}^{2}$$ denote the $$t^{th}$$ diagonal element of $$\bm{\Omega}$$, and $$\omega_{t}^{-2}$$ is the $$t^{th}$$ diagonal element of $$\bm{\Omega}^{-1}$$.  $$\bm{\Psi}$$ can be chosen as the diagonal matrix with $$t^{th}$$ diagonal element $$\omega_{t}^{-1}$$
\begin{eqnarray*}
\bm{\Omega}=\left[\begin{array}{cccc}
\omega_{1}^{2}&0&\cdots&0\\
0&\omega_{2}^{2}&\cdots&0\\
\vdots&\vdots&\ddots &\vdots\\
0&0&\cdots&\omega_{k}^{2}
\end{array}
\right]&,&\bm{\Omega}^{-1}=\left[\begin{array}{cccc}
\omega_{1}^{-2}&0&\cdots&0\\
0&\omega_{2}^{-2}&\cdots&0\\
\vdots&\vdots&\ddots &\vdots\\
0&0&\cdots&\omega_{k}^{-2}
\end{array}
\right]
\end{eqnarray*}
and
\begin{eqnarray*}
\bm{\Psi}&=&\left[\begin{array}{cccc}
\omega_{1}^{-1}&0&\cdots&0\\
0&\omega_{2}^{-1}&\cdots&0\\
\vdots&\vdots&\ddots &\vdots\\
0&0&\cdots&\omega_{k}^{-1}
\end{array}
\right]
\end{eqnarray*}
Thus we see that, for a typical observation, regression (\ref{eq7.3}) can be written as
\begin{eqnarray}
\label{eq7.13}
\omega_{t}^{-1}y_{t}&=&\omega_{t}^{-1}\bm{X}_{t}\bm{\beta}+\omega_{t}^{-1}u_{t}
\end{eqnarray}
the variance of the error term is clearly 1.

This special case of GLS estimation is often called \textbf{weighted least squares}, or \textbf{GLS}.

\section{Feasible Generalized Least Squares}
In practice, the covariance matrix $$\bm{\Omega}$$ is often not known even up to a scalar factor. This makes it impossible to compute GLS estimates.

However, it is reasonable to suppose that $$\bm{\Omega}$$, or $$\bm{\Delta}$$, depends in a known way on a vector of unknown parameters $$\bm{\gamma}$$. If so, it may be possible to estimate $$\bm{\gamma}$$ consistently, so as to obtain $$\bm{\Omega}(\hat{\gamma})$$. Then $$\bm{\Psi}(\hat{\gamma})$$ can be defined in (\ref{eq7.2}), and GLS estimates computed conditional on $$\bm{\Psi}(\hat{\gamma})$$. This type of procedure is called \textbf{feasible generalized least squares}, or \textbf{feasible GLS}.

For example, suppose the feasible GLS estimates of the linear regression model is
\begin{eqnarray}
\label{eq7.14}
y_{t}=\bm{X}_{t}\bm{\beta}+u_{t}&,&E(u_{t}^{2})=\exp(\bm{Z}_{t}\bm{\gamma})
\end{eqnarray}
where $$\bm{\beta}$$ and $$\bm{\gamma}$$ are a $$k$$-vector and an $$l$$-vector of unknown parameters, respectively. $$\bm{X}_{t}$$ and $$\bm{Z}_{t}$$ are comformably dimensioned row vectors of observation set on which we are conditioning. Some of the elements of $$\bm{Z}_{t}$$ may well belong to $$\bm{X}_{t}$$. The function $$\exp(\bm{Z}_{t}\bm{\gamma})$$ is an example of a \textbf{skedastic function}.

Steps to do FGLS
\begin{enumerate}
\item To obtain consistent estimates of $$\bm{\gamma}$$, we must first obtain consistent estimates of the error terms in (\ref{eq7.14}).

\item Computing OLS estimates $$\hat{\bm{\beta}}$$ allows us to calculate a vector of OLS residuals with typical element $$\hat{u}_{t}$$.

\item Run auxiliary linear regression
\begin{eqnarray}
\label{eq7.15}
\log \hat{u}_{t}^{2}&=&\bm{Z}_{t}\bm{\gamma}+v_{t}
\end{eqnarray}
to find OLS estimates $$\hat{\bm{\gamma}}$$.

\item Compute 
\begin{eqnarray*}
\hat{\omega}_{t}&=&\left(\exp(\bm{Z}_{t}\hat{\bm{\gamma}})\right)^{1/2}
\end{eqnarray*}

\item Feasible GLS of $$\bm{\beta}$$ are obtained by using ordinary least squares to estimate regression (\ref{eq7.13}), with the estimates $$\hat{\omega}_{t}$$ replacing the unknown $$\omega_{t}$$. 
\end{enumerate}

\subsection{Why FGLS Works}
Under suitable regularity conditions, it can be shown that this type of procedure leads a feasible GLS estimator $$\hat{\bm{\beta}}_{F}$$ that is consistent and asymptotically equivalent to the GLS estimator $$\hat{\bm{\beta}}_{GLS}$$.


\section{Heteroskedasticity}
If we have no information on the form of the skedastic function, it may be prudent to employ an HCCME, especially if the sample size is large. 

If we have information on the form of the skedastic function, we might well wish to use weighted least squares.

It makes no difference asymptotically whether the $$\omega_{t}$$ are known or merely estimated consistently, although it can certainly make a substantial difference in finite samples. Asymptotically, at least, the usual OLS covariance matrix is just as valid with feasible WLS as with WLS.

\subsection{Testing for Heteroskedasticity}
Before doing so, it is advisable to perform a \textbf{specification test} of the \underline{null hypothesis that the error terms are homoskedastic against whatever heteroskedastic} alternatives may seem reasonable.

White showed that, in a linear regression model, if $$E(u_{t}^{2})$$ is constant conditional on the squares and cross-products of all the regressors, then there is no need to use an HCCME. 

\section{Autoregressive Processes}
The error terms for nearby observations may be correlated, or may appear to be correlated. This phenomenon is most commonly encountered in models estimated with time-series data, where it is known as \textbf{serial correlation} or \textbf{autocorrelation}.

If there is reason to believe that serial correlation may be present
\begin{enumerate}
\item [\textbf{Step 1}:] Test the null hypothesis that the errors are serially uncorrelated against a plausible alternative that involves serial correlation.
\item [\textbf{Step 2}:] If evidence of serial correlation is found, estimate a model that accounts for it based on NLS and GLS.
\item [\textbf{Step 3}:] Verify that the model which accounts for serial correlation is compatible with the data
\end{enumerate}

\subsection{The AR(1) Process}
One of the simplest and most commonly used stochastic processes is the \textbf{first-order autoregressive process}, or \textbf{AR(1) process}, which can be written as
\begin{eqnarray}
\label{eq7.16}
u_{t}=\rho u_{t-1}+\varepsilon_{t}&,&\varepsilon_{t}\sim \mbox{IID}(0,\sigma_{\varepsilon})^{2},\ \ |\rho|<1
\end{eqnarray}
It is assumed that $$\varepsilon_{t}$$ is independent of $$\varepsilon_{s}$$ for all $$s\neq t$$, $$\varepsilon_{t}$$ is an innovation. 

$$|\rho|<1$$ is called a \textbf{stationarity condition}, because it is necessary for the AR(1) process to be \textbf{stationary}.

The \textbf{covariance stationarity/wide sense stationarity} is defined as $$E(u_{t})$$ and $$Var(u_{t}$$ exist and are independent of $$t$$ and if the covariance $$Cov(u_{t},u_{t-j})$$ is independent of $$t$$. 

We can then compute the variance of $$u_{t}$$ by substituting successively for $$u_{t-1}$$, $$u_{t-2}$$, $$u_{t-3}$$, and so on in (\ref{eq7.16})
\begin{eqnarray}
\label{eq7.17}
u_{t}&=&\varepsilon_{t}+\rho\varepsilon_{t-1}+\rho^{2}\varepsilon_{t-2}+\rho^{3}\varepsilon_{t-3}+\cdots
\end{eqnarray}

The variance of $$u_{t}$$ is seen to be
\begin{eqnarray}
\label{eq7.18}
\sigma_{u}^{2}&=&\sigma_{\varepsilon}^{2}+\rho^{2}\sigma_{\varepsilon}^{2}+\rho^{4}\sigma_{\varepsilon}^{2}+\rho^{6}\sigma_{\varepsilon}^{2}+\cdots=\frac{\sigma_{\varepsilon}^{2}}{1-\rho^{2}}
\end{eqnarray}

We can get that $$Var(u_{t})=\sigma_{\varepsilon}^{2}/(1-\rho^{2})$$ for all $$t$$.

The covariance for $$u_{t}$$ and $$u_{t-1}$$ of the AR(1) is
\begin{eqnarray*}
Cov(u_{t},u_{t-1})&=&E(u_{t}u_{t-1})\\
&=&E((\rho u_{t-1}+\varepsilon_{t})u_{t-1})\\
&=&\rho \sigma_{u}^{2}
\end{eqnarray*}

If the AR(1) process (\ref{eq7.16}) is stationary, the covariance matrix of the vector $$\bm{u}$$ can be written as
\begin{eqnarray}
\label{eq7.19}
\bm{\Omega}(\rho)&=&\frac{\sigma_{\varepsilon}^{2}}{1-\rho^{2}}\left[
\begin{array}{ccccc}
1&\rho&\rho^{2}&\cdots&\rho^{n-1}\\
\rho&1&\rho^{2}&\cdots&\rho^{n-2}\\
\vdots&\vdots&\vdots&&\vdots\\
\rho^{n-1}&\rho^{n-2}&\rho^{n-3}&\cdots&1
\end{array}
\right]
\end{eqnarray}


\subsection{Testing for Serial Correlation}
\begin{enumerate}
\item Run OLS and compute OLS residuals $$\hat{\bm{u}}=\bm{y}-\bm{X}
\hat{\bm{\beta}}_{OLS}$$
\item Auxiliary regression of OLS residuals $$\hat{\bu}$$ on lagged values
\begin{eqnarray*}
\hat{u}_{t}&=&\rho \hat{u}_{t-1}+\varepsilon_{t}
\end{eqnarray*}
\item $$t$$ test for $$\rho=0$$
\end{enumerate}

\subsection{FGLS for Autoregression Process}
If the $$u_{t}$$ follow a stationary AR(1) process, that is, if $$|\rho|<1$$ and $$Var(u_{t})=\sigma_{u}^{2}=\sigma_{\varepsilon}^{2}/(1-\rho^{2})$$, then the covariance matrix of the entire vector $$\hat{\bu}$$ is $$\bm{\Omega}(\rho)$$ defined in (\ref{eq7.19}).

\section{Moving Average Processes}
\subsection{The MA(1) Process}
\begin{eqnarray}
\label{eq7.20}
u_{t}=\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1}&,&\varepsilon_{t}\sim \mbox{IID}(0,\sigma_{\varepsilon}^{2})
\end{eqnarray}
$$u_{t}$$ is a weighted average of two successive innovations, $$\varepsilon_{t}$$ and $$\varepsilon_{t-1}$$.

The covariance matrix for an MA(1) process:
\begin{eqnarray*}
Var(u_{t})&=&E\left((\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1})^{2}\right)=\sigma_{\varepsilon}^{2}+\alpha_{1}^{2}\sigma_{\varepsilon}^{2}=(1+\alpha_{1}^{2})\sigma_{\varepsilon}^{2}\\
Cov(u_{t},u_{t-1})&=&E\left((\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1})(\varepsilon_{t-1}+\alpha_{1}\varepsilon_{t-2})\right)=\alpha_{1}\sigma_{\varepsilon}^{2}\\
Cov(u_{t},u_{t-j})&=&E\left((\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1})(\varepsilon_{t-j}+\alpha_{1}\varepsilon_{t-j-1})\right)=0
\end{eqnarray*}
Therefore, the covariance matrix of the entire vector $$\hat{\bu}$$ is
\begin{eqnarray}
\label{eq7.21}
\sigma_{\varepsilon}\bm{\Delta}(\alpha_{1})&=&\sigma_{\varepsilon}^{2}\left[\begin{array}{cccccc}
1+\alpha^{2}		&\alpha	&0	&\cdots	&0	&0	\\
\alpha	&1+\alpha^{2}		&\alpha	&\cdots	&0	&0\\
\vdots	&\vdots	&\vdots	&	&\vdots	&\vdots\\
0	&0	&0	&\cdots&\alpha	&1+\alpha^{2}
\end{array}\right]
\end{eqnarray}


\section{Models for Panel Data}
We restrict our attention to the linear regression model
\begin{eqnarray}
\label{eq7.10.1}
y_{it}&=&\bm{X}_{it}\bm{\beta}+u_{it}
\end{eqnarray}
where $$i=1,\dots, m$$, $$t=1,\dots, T$$, and $$\bm{X}_{it}$$ is a $$1\times k$$ vector.

If certain shocks affect the same cross-sectional unit at all points in time, the error terms $$u_{it}$$ and $$u_{is}$$ will be correlated for all $$t \neq s$$. Similarly, if certain shocks affect all cross-sectional units at the same point in time, the error terms $$u_{it}$$ and $$u_{j}$$t will be correlated for all $$i \neq j$$.

\subsection{Error-Components Models}
The idea is to specify the error term $$u_{it}$$ in (\ref{eq7.10.1}) as consisting of two or three separate shocks, each of which is assumed to be independent of the others.
\begin{eqnarray}
\label{eq7.10.2}
u_{it}&=&e_{t}+v_{i}+\varepsilon_{it}
\end{eqnarray}
It is generally assumed that the $$e_t$$ are independent across $$t$$, the $$v_i$$ are independent across $$i$$, and the $$\varepsilon_{it}$$, it are independent across all $$i$$ and $$t$$.

If the $$e_t$$ and $$v_i$$ are thought of as \textbf{fixed effects}, then they are treated as parameters to be estimated.

If they are thought of as \textbf{random effects}, then we must figure out the covariance matrix of the $$u_{it}$$ as functions of the variances of the $$e_{t}$$, $$v_{i}$$, and $$\varepsilon_{it}$$, and use feasible GLS.

\subsection{Random-Effects Estimation}
Random-effects estimation requires that the $$v_{i}$$ should be independent of $$\bm{X}$$. Then we have
\begin{eqnarray}
\label{eq7.10.3}
E(u_{it}|\bm{X})&=&E(v_{i}+\varepsilon_{it}|\bm{X})=0
\end{eqnarray}

However $$u_{it}$$ are not IID. Assuming that $$v_{i}\sim IID(0, \sigma^{2}_{v})$$, $$e_{t}$$, and shocks are independent, we find that
\begin{eqnarray*}
Var(u_{it})&=&\sigma_{v}^{2}+\sigma_{\varepsilon}^{2}\\
Cov(u_{it}, u_{is})&=&\sigma_{v}^{2}\\
Cov(u_{it}, u_{js})&=&0
\end{eqnarray*}

These define the elements of the covariance matrix $$\bm{\Omega}$$
\begin{eqnarray*}
\bm{\Omega}&=&\left[\begin{array}{cccc}
\bm{\Sigma}&\bm{0}&\cdots&\bm{0}\\
\bm{0}&\bm{\Sigma}&\cdots&\bm{0}\\
\vdots&\vdots&&\vdots\\
\bm{0}&\bm{0}&\cdots&\bm{\Sigma}\\
\end{array}\right]
\end{eqnarray*}
where 
\begin{eqnarray}
\label{eq7.10.4}
\bm{\Sigma}&=&\sigma_{\varepsilon}^{2}I_{T}+\sigma_{v}^{2}\bm{\iota}\bm{\iota}^{T}
\end{eqnarray}

\subsection{Properties of OLS estimator in Random Effects Model}
Under the assumption that $$E(\bu|\x)=0$$, the OLS estimator in random effects model is unbiased and consistent. However, since the error term is heteroskedastic, OLS estimator is not efficient.





































