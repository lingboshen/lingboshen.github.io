---
layout: post
title: Davidson and MacKinnon Chapter 3&mdash;The Statistical Properties of Ordinary Least Squares  
date: 2022-10-23 11:12:00-0400
description: The third chapter of Econometric Theory and Methods
tags: note econometics
categories: 
---

# Introduction

$$\begin{eqnarray}
\label{eqo1}
y=X\be+u&,& u\sim \mbox{IID}(0,\sigma^{2}I)
\end{eqnarray}$$

where $$y$$ and $$u$$ are $$n$$-vectors, $$X$$ is an $$n\times k$$ matrix, and $$\be$$ is a $$k$$-vector.

And the OLS estimator is

$$\begin{eqnarray}
\label{eqo2}
\hat{\beta}&=&(X^{T}X)^{-1}X^{T}y
\end{eqnarray}$$


# Are OLS Estimator Unbiased?

$$\begin{eqnarray}
\nonumber
\hat{\beta}&=&(X^{T}X)^{-1}X^{T}(X\beta+u)\\
\label{eqo3}
&=&\beta+(X^{T}X)^{-1}X^{T}u
\end{eqnarray}$$

\framebox{\textbf{Assumption}: $$E(X|u)=0$$}

Take conditional expectation with respect to (\ref{eqo3}), then it becomes 

$$\begin{eqnarray}
\nonumber
E(\hat{\beta}|X)&=&\beta+E[(X^{T}X)^{-1}X^{T}u|X]\\\nonumber
&=&\beta+(X^{T}X)^{-1}X^{T}E(u|X)\\
\label{eqo4}
&=&\beta
\end{eqnarray}$$

By Law of Iterated Expectations, we have

$$\begin{eqnarray}
\label{eqo5}
E(\hat{\beta})&=&E\left(E(\hat{\beta}|X)\right)=E(\beta)=\beta
\end{eqnarray}$$


The Assumption 1 maybe too strong for \textbf{time-series data}. We can make following assumption

$$\begin{eqnarray}
\label{eqo6}
E(u_{t}|X_{t})&=&0
\end{eqnarray}$$

We refer (\ref{eqo6}) as a \textbf{predeterminedness} condition.

# Are OLS Estimator Consistent
If the sample size is large enough, the estimate will be close to the true value.
\paragraph{Probability Limits}

$$\begin{eqnarray}
\label{eqo7}
\plim_{n\to\infty} a(y^{n})&=&a_{0}\\
\label{eqo8}
\lim_{n\to\infty}\Pr\left(\norm{a(y^{n})-a_{0}}<\varepsilon\right)&=&1
\end{eqnarray}$$

\textbf{Law of large numbers (LLN)}: suppose that $$\bar{x}$$ is the sample mean of $$x_{t}$$, $$t=1,\dots, n$$, a sequence of random variables, each with expectation $$\mu$$. Then provided the $$x_{t}$$ are independent, a law of large numbers would state that

$$\begin{eqnarray}
\label{eqo9}
\plim_{n\to\infty}\bar{x}&=&\plim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n}x_{t}=\mu
\end{eqnarray}$$

$$\bar{x}$$ has a nonstochastic plim which is equal to the common expectation of each of the $$x_{t}$$.

\paragraph{OLS is consistent}
**Assumption**: 

$$\plim_{n\to\infty}\frac{1}{n}X^{T}X=S_{X^{T}X}$$}

**Assumption**: 

$$E(u_{t}|x_{t})=0$$}


$$\begin{eqnarray}
\nonumber
\plim_{n\to\infty}\hat{\beta}&=&\plim_{n\to\infty}\left(\beta+(X^{T}X)^{-1}X^{T}u\right)\\\nonumber
&=&\beta+\plim_{n\to\infty}(\frac{1}{n}X^{T}X)^{-1}+\plim_{n\to\infty}\frac{1}{n}X^{T}u\\\nonumber
&=&\beta+S_{X^{T}X}^{-1}\plim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n} X_{t}^{T}u_{t} \\\nonumber
&=&\beta+S_{X^{T}X}^{-1} E(X_{t}^{T}u_{t}) \\\nonumber
&=&\beta+S_{X^{T}X}^{-1} E(E(X_{t}^{T}u_{t})|X_{t}) \\\nonumber
&=&\beta+S_{X^{T}X}^{-1} E(X_{t}^{T}E(u_{t})|X_{t}) \\
\label{eqo10}
&=&\beta
\end{eqnarray}$$


# The Covariance Matrix of the OLS Estimator
The full covariance matrix $$Var(\bm{b})$$ can be expressed by

$$\begin{eqnarray}
\label{eqo11}
Var(\bm{b})&=&E\left(\left(\bm{b}-E(\bm{b})\right)\left(\bm{b}-E(\bm{b})\right)^{T}\right)
\end{eqnarray}$$


$$Var(\bm{b})$$ is symmetric and positive semidefinite.

\subsection{The OLS Covariance Matrix}
If the error terms are IID, and have the same variance $$\sigma^{2}$$, and the covariance of any pair of them is zero.

$$\begin{eqnarray}
\label{eqo12}
Var(\bm{u})&=&E(\bm{u}\bm{u}^{T})=\sigma^{2}I
\end{eqnarray}$$


If we assume that $$X$$ is exogenous,

$$\begin{eqnarray*}
E((\hat{\beta}-\beta)(\hat{\beta}-\beta)^{T}|X)&=&E((X^{T}X)^{-1}X^{T}u)(X^{T}X)^{-1}X^{T}u)^{T}|X)\\
&=&E((X^{T}X)^{-1}X^{T}uu^{T}X(X^{T}X)^{-1}|X)\\
&=&(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}\sigma^{2}I\\
&=&\sigma^{2}(X^{T}X)^{-1}
\end{eqnarray*}$$


\paragraph{Linear Functions of Parameter Estimates}

$$\begin{eqnarray*}
Var(\bm{\omega}^{T}\hat{\beta})&=&\bm{\omega}^{T}Var(\hat{\beta})\bm{\omega}\\
&=&\bm{\omega}^{T}\left(\sigma^{2}(X^{T}X)^{-1}\right)\bm{\omega}\\
\end{eqnarray*}$$


\paragraph{Estimating the Variance of the Error Terms}

$$\begin{eqnarray}
\label{eqo13}
s^{2}&=&\frac{1}{n-k}\sum_{t=1}^{n}\hat{u}_{t}^{2}
\end{eqnarray}$$

We obtain an unbiased estimate of $$Var(\hat{\beta})$$

$$\begin{eqnarray}
\label{eqo14}
Var(\hat{\beta})&=&s^{2}(X^{T}X)^{-1}=\left(\frac{1}{n-k}\sum_{t=1}^{n}\hat{u}_{t}^{2}\right)(X^{T}X)^{-1}
\end{eqnarray}$$


# Efficiency of the OLS Estimator
\begin{proposition} \textbf{Gauss-Markov Theorem}: If it is assumed that $$E(u|X)=0$$ and $$E(uu^{T})=\sigma^{2}I$$ in the linear regression model, then the OLS estimator $$\hat{\beta}$$ is more efficient than any other linear unbiased estimator $$\tilde{\beta}$$, in sense that $$Var(\tilde{\beta})-Var(\hat{\beta})$$ is a positive semidefinite matrix.
\end{proposition}

# Goodness of Fit
**Adjusted $$R^{2}$$**

$$\begin{eqnarray}
\label{eqo15}
\bar{R}_{2}&=&1-\frac{\frac{1}{n-k}\sum_{t=1}^{n}\hat{u}_{t}^{2}}{\frac{1}{n-1}\sum_{t=1}^{n}(y_{t}-\bar{y})^{2}}=1-\frac{\frac{1}{n-k}y^{T}\m_{X}y}{\frac{1}{n-1}y^{T}\m_{\iota}y}
\end{eqnarray}$$

