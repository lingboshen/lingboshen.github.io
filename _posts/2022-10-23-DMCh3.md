---
layout: post
title: Davidson and MacKinnon Chapter 3&mdash;
date: 2022-10-23 11:12:00-0400
description: The third chapter of Econometric Theory and Methods
tags: note econometics
categories: 
---

# Introduction

$$\begin{eqnarray}
\label{eqo1}
\y=\x\be+\bu&,& \bu\sim \mbox{IID}(\bm{0},\sigma^{2}\bm{I})
\end{eqnarray}$$

where $\y$ and $\bu$ are $n$-vectors, $\x$ is an $n\times k$ matrix, and $\be$ is a $k$-vector.

And the OLS estimator is

$$\begin{eqnarray}
\label{eqo2}
\hat{\be}&=&(\x^{T}\x)^{-1}\x^{T}\y
\end{eqnarray}$$


\section{Are OLS Estimator Unbiased?}

$$\begin{eqnarray}
\nonumber
\hat{\be}&=&(\x^{T}\x)^{-1}\x^{T}(\x\be+\bu)\\
\label{eqo3}
&=&\be+(\x^{T}\x)^{-1}\x^{T}\bu
\end{eqnarray}$$

\framebox{\textbf{Assumption}: $E(\x|\bu)=0$}

Take conditional expectation with respect to (\ref{eqo3}), then it becomes 

$$\begin{eqnarray}
\nonumber
E(\hat{\be}|\x)&=&\be+E[(\x^{T}\x)^{-1}\x^{T}\bu|\x]\\\nonumber
&=&\be+(\x^{T}\x)^{-1}\x^{T}E(\bu|\x)\\
\label{eqo4}
&=&\be
\end{eqnarray}$$

By Law of Iterated Expectations, we have

$$\begin{eqnarray}
\label{eqo5}
E(\hat{\be})&=&E\left(E(\hat{\be}|\x)\right)=E(\be)=\be
\end{eqnarray}$$


The Assumption 1 maybe too strong for \textbf{time-series data}. We can make following assumption

$$\begin{eqnarray}
\label{eqo6}
E(u_{t}|\x_{t})&=&0
\end{eqnarray}$$

We refer (\ref{eqo6}) as a \textbf{predeterminedness} condition.

\section{Are OLS Estimator Consistent}
If the sample size is large enough, the estimate will be close to the true value.
\paragraph{Probability Limits}

$$\begin{eqnarray}
\label{eqo7}
\plim_{n\to\infty} \bm{a}(\bm{y}^{n})&=&\bm{a}_{0}\\
\label{eqo8}
\lim_{n\to\infty}\Pr\left(\norm{\bm{a}(\bm{y}^{n})-\bm{a}_{0}}<\varepsilon\right)&=&1
\end{eqnarray}$$


\textbf{Law of large numbers (LLN)}: suppose that $\bar{x}$ is the sample mean of $x_{t}$, $t=1,\dots, n$, a sequence of random variables, each with expectation $\mu$. Then provided the $x_{t}$ are independent, a law of large numbers would state that

$$\begin{eqnarray}
\label{eqo9}
\plim_{n\to\infty}\bar{x}&=&\plim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n}x_{t}=\mu
\end{eqnarray}$$

$\bar{x}$ has a nonstochastic plim which is equal to the common expectation of each of the $x_{t}$.

\paragraph{OLS is consistent}
\framebox{\textbf{Assumption}: $\plim_{n\to\infty}\frac{1}{n}\x^{T}\x=\bm{S}_{\x^{T}\x}$}

\framebox{\textbf{Assumption}: $E(u_{t}|x_{t})=0$}


$$\begin{eqnarray}
\nonumber
\plim_{n\to\infty}\hat{\be}&=&\plim_{n\to\infty}\left(\be+(\x^{T}\x)^{-1}\x^{T}\bu\right)\\\nonumber
&=&\be+\plim_{n\to\infty}(\frac{1}{n}\x^{T}\x)^{-1}+\plim_{n\to\infty}\frac{1}{n}\x^{T}\bu\\\nonumber
&=&\be+\bm{S}_{\x^{T}\x}^{-1}\plim_{n\to\infty}\frac{1}{n}\sum_{t=1}^{n} \x_{t}^{T}u_{t} \\\nonumber
&=&\be+\bm{S}_{\x^{T}\x}^{-1} E(\x_{t}^{T}u_{t}) \\\nonumber
&=&\be+\bm{S}_{\x^{T}\x}^{-1} E(E(\x_{t}^{T}u_{t})|\x_{t}) \\\nonumber
&=&\be+\bm{S}_{\x^{T}\x}^{-1} E(\x_{t}^{T}E(u_{t})|\x_{t}) \\
\label{eqo10}
&=&\be
\end{eqnarray}$$


\section{The Covariance Matrix of the OLS Estimator}
The full covariance matrix $Var(\bm{b})$ can be expressed by

$$\begin{eqnarray}
\label{eqo11}
Var(\bm{b})&=&E\left(\left(\bm{b}-E(\bm{b})\right)\left(\bm{b}-E(\bm{b})\right)^{T}\right)
\end{eqnarray}$$


$Var(\bm{b})$ is symmetric and positive semidefinite.

\subsection{The OLS Covariance Matrix}
If the error terms are IID, and have the same variance $\sigma^{2}$, and the covariance of any pair of them is zero.

$$\begin{eqnarray}
\label{eqo12}
Var(\bm{u})&=&E(\bm{u}\bm{u}^{T})=\sigma^{2}\bm{I}
\end{eqnarray}$$


If we assume that $\x$ is exogenous,

$$\begin{eqnarray*}
E((\hat{\be}-\be)(\hat{\be}-\be)^{T}|\x)&=&E((\x^{T}\x)^{-1}\x^{T}\bu)(\x^{T}\x)^{-1}\x^{T}\bu)^{T}|\x)\\
&=&E((\x^{T}\x)^{-1}\x^{T}\bu\bu^{T}\x(\x^{T}\x)^{-1}|\x)\\
&=&(\x^{T}\x)^{-1}\x^{T}\x(\x^{T}\x)^{-1}\sigma^{2}\bm{I}\\
&=&\sigma^{2}(\x^{T}\x)^{-1}
\end{eqnarray*}$$


\paragraph{Linear Functions of Parameter Estimates}

$$\begin{eqnarray*}
Var(\bm{\omega}^{T}\hat{\be})&=&\bm{\omega}^{T}Var(\hat{\be})\bm{\omega}\\
&=&\bm{\omega}^{T}\left(\sigma^{2}(\x^{T}\x)^{-1}\right)\bm{\omega}\\
\end{eqnarray*}$$


\paragraph{Estimating the Variance of the Error Terms}

$$\begin{eqnarray}
\label{eqo13}
s^{2}&=&\frac{1}{n-k}\sum_{t=1}^{n}\hat{u}_{t}^{2}
\end{eqnarray}$$

We obtain an unbiased estimate of $Var(\hat{\be})$

$$\begin{eqnarray}
\label{eqo14}
Var(\hat{\be})&=&s^{2}(\x^{T}\x)^{-1}=\left(\frac{1}{n-k}\sum_{t=1}^{n}\hat{u}_{t}^{2}\right)(\x^{T}\x)^{-1}
\end{eqnarray}$$


\section{Efficiency of the OLS Estimator}
\begin{proposition} \textbf{Gauss-Markov Theorem}: If it is assumed that $E(\bu|\x)=\bm{0}$ and $E(\bu\bu^{T})=\sigma^{2}\bm{I}$ in the linear regression model, then the OLS estimator $\hat{\be}$ is more efficient than any other linear unbiased estimator $\tilde{\be}$, in sense that $Var(\tilde{\be})-Var(\hat{\be})$ is a positive semidefinite matrix.
\end{proposition}

\section{Goodness of Fit}
\textbf{Adjusted $R^{2}$}

$$\begin{eqnarray}
\label{eqo15}
\bar{R}_{2}&=&1-\frac{\frac{1}{n-k}\sum_{t=1}^{n}\hat{u}_{t}^{2}}{\frac{1}{n-1}\sum_{t=1}^{n}(y_{t}-\bar{y})^{2}}=1-\frac{\frac{1}{n-k}\y^{T}\m_{\x}\y}{\frac{1}{n-1}\y^{T}\m_{\iota}\y}
\end{eqnarray}$$

