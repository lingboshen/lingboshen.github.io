<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lingbo  Shen | Time Series 3&mdash;ARMA</title>
    <meta name="author" content="Lingbo  Shen" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />

    <meta name="google-site-verification" content="45VVBTzbWI9CPrT2UtIdTtL6imAnvMgtV1VttR2zNGI" />
    
    <!-- Bootstrap & MDB -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://lingboshen.github.io/blog/2022/TS3/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://lingboshen.github.io/"><span class="font-weight-bold">Lingbo</span>   Shen</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">Home</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Research</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>
<!-- Chinese -->
              <li class="nav-item ">
                <a class="nav-link" href="/chinese/">Chinese/中文</a>
              </li>

              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Time Series 3—ARMA</h1>
    <p class="post-meta">October 23, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/note">
          <i class="fas fa-hashtag fa-sm"></i> note</a>  
          <a href="/blog/tag/time-series">
          <i class="fas fa-hashtag fa-sm"></i> time-series</a>  
          <a href="/blog/tag/econometics">
          <i class="fas fa-hashtag fa-sm"></i> econometics</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>The autoregressive moving average (ARMA) model</p>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;a_{0}+\sum_{i=1}^{p}a_{i}y_{t-i}+\sum_{i=0}^{q}\beta_{i}\varepsilon_{t-i}
\end{eqnarray*}\]

<h1 id="stationarity">Stationarity</h1>
<p>A stochastic process is <em>covariance stationary</em> or <em>weakly stationary</em> if for all \(t\) and \(s\)</p>

\[\begin{eqnarray*}
E(y_{t})&amp;=&amp;E(y_{t-s})=\mu\\
E\left[(y_{t}-\mu)^{2}\right]&amp;=&amp;E\left[(y_{t-s}-\mu)^{2}\right]=\sigma_{y}^{2}=\gamma_{0}\\
E\left[(y_{t}-\mu)(y_{t-s}-\mu)\right]&amp;=&amp;E\left[(y_{t-j}-\mu)(y_{t-j-s}-\mu)\right]=\gamma_{s}
\end{eqnarray*}\]

<p>If a process is covariance stationary, the covariance between \(y_{t}\) and \(y_{t-s}\) depends only on \(s\), the length of time separating the observations. It follows that for a covariance stationary process, \(\gamma_{s}\) and \(\gamma_{-s}\) would represent the same magnitude.</p>

<p>For a covariance stationary series, we can define the autocorrelation between \(y_{t}\) and \(y_{t-s}\)</p>

\[\begin{eqnarray*}
\rho_{s}&amp;=&amp;\frac{\gamma_{s}}{\gamma_{0}}
\end{eqnarray*}\]

<p>where \(\gamma_{0}\) is the variance of \(y_{t}\)</p>

<h1 id="ergodicity">Ergodicity</h1>
<p>Imagine a battery of \(I\) computers generating sequences \(\{y_{t}^{(1)}\}_{t=-\infty}^{\infty}\), \(\{y_{t}^{(2)}\}_{t=-\infty}^{\infty}\), \(\dots\), \(\{y_{t}^{(I)}\}_{t=-\infty}^{\infty}\) and consider selecting the observation associated with date \(t\) from each sequence:
\(\{y_{t}^{(1)},y_{t}^{(2)},\dots,y_{t}^{(I)}\}\)
This would be described as a sample of \(I\) realizations of the random variable \(Y_{t}\). The expectation of the \(t\)th observation of a time series refers to the mean of the probability distribution</p>

\[\begin{eqnarray*}
E(Y_{t})&amp;=&amp;\int_{-\infty}^{\infty}y_{t}f_{Y_{t}}(y_{t})dy_{t}
\end{eqnarray*}\]

<p>We might view this as the probability limit of the ensemble average</p>

\[\begin{eqnarray*}
E(Y_{t})&amp;=&amp;\mathop{plim}_{I\to\infty}\frac{1}{I}\sum_{i=1}^{I}Y_{t}^{(i)}
\end{eqnarray*}\]

<p>The above expectations of a time series in terms of ensemble averages may seem a bit contrived. Usually we have a single realization of size \(T\) from the process 
\(\{y_{1}^{(1)},y_{2}^{(1)},\dots,y_{T}^{(1)}\}\)
From these observations we would calculate the sample mean \(\bar{y}\), which is a time average</p>

\[\begin{eqnarray*}
\bar{y}&amp;=&amp;\frac{1}{T}\sum_{t=1}^{T}y_{t}^{(1)}
\end{eqnarray*}\]

<p>A covariance stationary process is said to be <em>ergodic for the mean</em> if \(\bar{y}\) converges in probability to \(E(Y_{t})\) as \(T\to\infty\).</p>

<h1 id="moving-average-processes">Moving Average Processes</h1>
<h2 id="the-first-order-moving-average-process">The First-Order Moving Average Process</h2>
<p>Let \(\{\varepsilon_{t}\}\) be white noise and consider the process</p>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;\mu+\varepsilon_{t}+\theta\varepsilon_{t-1}
\end{eqnarray*}\]

<p>where \(\mu\) and \(\theta\) could be any constants. This time series is called a first-order moving average process, denoted \(MA(1)\).</p>

<p><strong>Expectation</strong>
The expectation of \(y_{t}\) is</p>

\[\begin{eqnarray*}
E(y_{t})&amp;=&amp;E(\mu+\varepsilon_{t}+\theta\varepsilon_{t-1})=\mu+E(\varepsilon)+\theta E(\varepsilon_{t-1})=\mu
\end{eqnarray*}\]

<p><strong>Variance</strong>
The variance of \(y_{t}\) is</p>

\[\begin{eqnarray*}
E(y_{t}-\mu)^{2}&amp;=&amp;E(\varepsilon_{t}+\theta\varepsilon_{t-1})^{2}\\
&amp;=&amp;E\left(\varepsilon_{t}^{2}+2\theta\varepsilon_{t}\varepsilon_{t-1}+\theta^{2}\varepsilon_{t-1}^{2}\right)\\
&amp;=&amp;\sigma^{2}+0+\theta^{2}\sigma^{2}\\
&amp;=&amp;(1+\theta^{2})\sigma^{2}
\end{eqnarray*}\]

<p><strong>Autocovariance</strong>
The first autocovariance of \(y_{t}\) is</p>

\[\begin{eqnarray*}
E(y_{t}-\mu)(y_{t-1}-\mu)&amp;=&amp;E(\varepsilon_{t}+\theta\varepsilon_{t-1})(\varepsilon_{t-1}+\theta\varepsilon_{t-2})\\
&amp;=&amp;E\left(\varepsilon_{t}\varepsilon_{t-1}+\theta\varepsilon_{t-1}^{2}+\theta\varepsilon_{t}\varepsilon_{t-2}+\theta^{2}\varepsilon_{t-1}\varepsilon_{t-2}\right)\\
&amp;=&amp;0+\theta\sigma^{2}+0+0\\
&amp;=&amp;\theta^{2}\sigma^{2}
\end{eqnarray*}\]

<p>Higher autocovariances are all zero. For all \(j&gt;1\)</p>

\[\begin{eqnarray*}
E(y_{t}-\mu)(y_{t-j}-\mu)&amp;=&amp;0
\end{eqnarray*}\]

<p><strong>Autocorrelation</strong>
The \(j\)th autocorrelation of a covariance stationary process is \(\rho_{j}=\frac{\gamma_{j}}{\gamma_{0}}\)</p>

\[\begin{eqnarray*}
\rho_{1}&amp;=&amp;\frac{\gamma_{1}}{\gamma_{0}}=\frac{\theta^{2}\sigma^{2}}{(1+\theta^{2})\sigma^{2}}=\frac{\theta^{2}}{1+\theta^{2}}\\
\rho_{j}&amp;=&amp;0, j&gt;1
\end{eqnarray*}\]

<h2 id="the-qth-order-moving-average-process">The \(q\)th-Order Moving Average Process</h2>
<p>A \(q\)th-order moving average process, denoted \(MA(q)\), is characterized by</p>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;\mu+\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q}
\end{eqnarray*}\]

<p><strong>Expectation</strong>
The expectation of \(y_{t}\) is</p>

\[\begin{eqnarray*}
E(y_{t})&amp;=&amp;E(\mu)+E(\varepsilon_{t})+\theta_{1}E(\varepsilon_{t-1})+\theta_{2}E(\varepsilon_{t-2})+\cdots+\theta_{q}E(\varepsilon_{t-q})=\mu
\end{eqnarray*}\]

<p><strong>Variance</strong>
The variance of \(y_{t}\) is</p>

\[\begin{eqnarray*}
E(y_{t}-\mu)^{2}&amp;=&amp;E(\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q})^{2}\\
&amp;=&amp;E(\varepsilon_{t})^{2}+E(\theta_{1}\varepsilon_{t-1})^{2}+E(\theta_{2}\varepsilon_{t-2})^{2}+\cdots+E(\theta_{q}\varepsilon_{t-q})^{2}\\
&amp;=&amp;\sigma^{2}+\theta_{1}^{2}\sigma^{2}+\theta_{2}^{2}\sigma^{2}+\cdot+\theta_{q}^{2}\sigma^{2}\\
&amp;=&amp;(1+\theta_{1}^{2}+\theta_{2}^{2}+\cdot+\theta_{q}^{2})\sigma^{2}
\end{eqnarray*}\]

<p><strong>Autocovariance</strong>
The autocovariance of \(y_{t}\) is</p>

\[\begin{eqnarray*}
E(y_{t}-\mu)(y_{t-j}-\mu)&amp;=&amp;E(\varepsilon_{t}+\theta_{1}\varepsilon_{t-1}+\theta_{2}\varepsilon_{t-2}+\cdots+\theta_{q}\varepsilon_{t-q})\\
&amp;&amp;(\varepsilon_{t-j}+\theta_{1}\varepsilon_{t-j-1}+\theta_{2}\varepsilon_{t-j-2}+\cdots+\theta_{q}\varepsilon_{t-j-q})\\
&amp;=&amp;E\left(\theta_{j}\varepsilon_{t-j}^{2}+\theta_{j+1}\theta_{1}\varepsilon_{t-j-1}^{2}+\theta_{j+2}\theta_{2}\varepsilon_{t-j-2}^{2}+\cdots+\theta_{q}\theta_{q-j}\varepsilon_{t-q}^{2}\right)\\
&amp;=&amp;\left(\theta_{j}+\theta_{j+1}\theta_{1}+\theta_{j+2}\theta_{2}+\cdots+\theta_{q}\theta_{q-j}\right)\sigma^{2}, j=1,2,\dots,q
\end{eqnarray*}\]

<p>For all \(j&gt;q\)</p>

\[\begin{eqnarray*}
E(y_{t}-\mu)(y_{t-j}-\mu)&amp;=&amp;0
\end{eqnarray*}\]

<h2 id="the-infinite-order-moving-average-process">The Infinite-Order Moving Average Process</h2>
<p>Consider the process when \(q\to\infty\)</p>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;\mu+\sum_{j=0}^{\infty}\psi_{j}\varepsilon_{t-j}=\mu+\psi_{0}\varepsilon_{t}+\psi_{1}\varepsilon_{t-1}+\psi_{2}\varepsilon_{t-2}+\cdots
\end{eqnarray*}\]

<p>This could be described as an \(MA(\infty)\) process.</p>

<p>The \(MA(\infty)\) process is covariance stationary if it is square summable</p>

\[\begin{eqnarray*}
\sum_{j=0}^{\infty}\psi_{j}^{2}&amp;&lt;&amp;\infty
\end{eqnarray*}\]

<p>It is often to work with a slightly stronger condition called absolutely summable</p>

\[\begin{eqnarray*}
\sum_{j=0}^{\infty}|\psi_{j}|&amp;&lt;&amp;\infty
\end{eqnarray*}\]

<p><strong>Expectation</strong>
The mean of an \(MA(\infty)\) process with absolutely summable is</p>

\[\begin{eqnarray*}
E(y_{t})&amp;=&amp;\lim_{T\infty}E(\mu+\psi_{0}\varepsilon_{t}+\psi_{1}\varepsilon_{t-1}+\psi_{2}\varepsilon_{t-2}+\cdots+\psi_{T}\varepsilon_{t-T})=\mu
\end{eqnarray*}\]

<p><strong>Autocovariance</strong>
The autocovariance of an \(MA(\infty)\) process with absolutely summable is</p>

\[\begin{eqnarray*}
\gamma_{0}&amp;=&amp;E(y_{t}-\mu)^{2}\\
&amp;=&amp;\lim_{T\infty}E(\psi_{0}\varepsilon_{t}+\psi_{1}\varepsilon_{t-1}+\psi_{2}\varepsilon_{t-2}+\cdots+\psi_{T}\varepsilon_{t-T})^{2}\\
&amp;=&amp;\lim_{T\to\infty}(\psi_{0}^{2}+\psi_{1}^{2}+\psi_{2}^{2}+\cdots+\psi_{T}^{2})\sigma^{2}\\
\gamma_{j}&amp;=&amp;E(y_{t}-\mu)(y_{t-j}-\mu)\\
&amp;=&amp;(\psi_{j}\psi_{0}+\psi_{j+1}\psi_{1}+\psi_{j+2}\psi_{2}+\psi_{j+3}\psi_{3}+\cdots)\sigma^{2}
\end{eqnarray*}\]

<h1 id="autoregressive-processes">Autoregressive Processes</h1>
<h2 id="the-first-order-autoregressive-process">The First-Order Autoregressive Process</h2>
<p>A first-order autoregressive, denoted \(AR(1)\), satisfies the following difference equation</p>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;c+\phi y_{t-1}+\varepsilon_{t}
\end{eqnarray*}\]

<table>
  <tbody>
    <tr>
      <td>When $$</td>
      <td>\phi</td>
      <td>&lt;1$$, this process is covariance stationary. It can be rewritten as</td>
    </tr>
  </tbody>
</table>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;c+\phi y_{t-1}+\varepsilon_{t}\\
&amp;=&amp;c+\varepsilon_{t}+\phi (c+\varepsilon_{t-1})+\phi^{2} y_{t-2}\\
&amp;=&amp;c+\varepsilon_{t}+\phi (c+\varepsilon_{t-1})+\phi^{2} (c+\varepsilon_{t-2})+\phi^{3} y_{t-3}\\
&amp;=&amp;c+\varepsilon_{t}+\phi (c+\varepsilon_{t-1})+\phi^{2} (c+\varepsilon_{t-2})+\phi^{3} (c+\varepsilon_{t-3})+\phi^{4} y_{t-4}\\
&amp;=&amp;c+\varepsilon_{t}+\phi (c+\varepsilon_{t-1})+\phi^{2} (c+\varepsilon_{t-2})+\phi^{3} (c+\varepsilon_{t-3})+\cdots\\
&amp;=&amp;(c+\phi c+\phi^{2}c+\phi^{3}c+\cdots)+\varepsilon_{t}+\phi\varepsilon_{t-1}+\phi^{2}\varepsilon_{t-2}+\phi^{3}\varepsilon_{t-3}+\cdots\\
&amp;=&amp;\frac{c}{1-\phi}+\varepsilon_{t}+\phi\varepsilon_{t-1}+\phi^{2}\varepsilon_{t-2}+\phi^{3}\varepsilon_{t-3}+\cdots
\end{eqnarray*}\]

<p>We can derive the expectation and autocovariance of \(AR(1)\) by the above corresponding \(MA(\infty)\) process. We also can derive them by assuming \(AR(1)\) process is covariance stationary.</p>

<p><strong>Expectation</strong>
Taking expectations both sides</p>

\[\begin{eqnarray*}
E(y_{t})&amp;=&amp;c+\phi E(y_{t-1})+E(\varepsilon_{t})\\
\mu&amp;=&amp;c+\phi\mu\\
\mu&amp;=&amp;\frac{c}{1-\phi}
\end{eqnarray*}\]

<p><strong>Autocovariance</strong></p>

<h2 id="the-qth-order-autoregressive-process">The \(q\)th-Order Autoregressive Process</h2>

<h1 id="the-autocorrelation-function">The Autocorrelation Function</h1>
<p>Autocorrelation function (ACF) and the partial autocorrelation function (PACF) are useful to determine the type of time series data.</p>

<p>For AR(1) model</p>

<ul>
  <li>Method 1</li>
</ul>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;a_{0}+a_{1}y_{t-1}+\varepsilon_{t}\ \ assume \ stationary\\
E(y_{t})&amp;=&amp;a_{0}+a_{1}E(y_{t-1})+E(\varepsilon_{t})\\
\Rightarrow\mu&amp;=&amp;a_{0}+a_{1}\mu\\
\Rightarrow\mu&amp;=&amp;\frac{a_{0}}{1-a_{1}}\\
Var(y_{t})&amp;=&amp;a_{1}^{2}Var(y_{t-1})+Var(\varepsilon_{t})\\
\Rightarrow \gamma_{0}&amp;=&amp;a_{1}^{2}\gamma_{0}+\sigma^{2}\\
\Rightarrow \gamma_{0}&amp;=&amp;\frac{\sigma^{2}}{1-a_{1}^{2}}\\
Cov(y_{t}, y_{t-s})&amp;=&amp;Cov(a_{0}+a_{1}y_{t-1}+\varepsilon_{t}, a_{0}+a_{1}y_{t-s-1}+\varepsilon_{t-s})\\
			    &amp;=&amp;Cov(a_{1}y_{t-1}+\varepsilon_{t}, a_{1}y_{t-s-1}+\varepsilon_{t-s})\\
\Rightarrow \gamma_{s}&amp;=&amp;a_{1}^{2}\gamma_{s}+a_{1}^{s}\sigma^{2}\\
\Rightarrow \gamma_{s}&amp;=&amp;\frac{a_{1}^{s}\sigma^{2}}{1-a_{1}^{2}}\\
\end{eqnarray*}\]

<p>So the autocorrelation function for AR(1)</p>

\[\begin{eqnarray*}
\rho_{s}&amp;=&amp;\frac{\gamma_{s}}{\gamma_{0}}\\
&amp;=&amp;a_{1}^{s}
\end{eqnarray*}\]

<ul>
  <li>Method 2 
If the process started at time zero</li>
</ul>

\[\begin{eqnarray*}
y_{t}&amp;=&amp;a_{0}\sum_{i=0}^{t-1}a_{1}^{i}+a_{1}^{t}y_{0}+\sum_{i=0}^{t-1}a_{1}^{i}\varepsilon_{t-i}
\end{eqnarray*}\]

<p>Take the expectation of \(y_{t}\) and \(y_{t+s}\)</p>

\[\begin{eqnarray*}
E(y_{t})&amp;=&amp;a_{0}\sum_{i=0}^{t-1}a_{1}^{i}+a_{1}^{t}y_{0}\\
E(y_{t+s})&amp;=&amp;a_{0}\sum_{i=0}^{t+s-1}a_{1}^{i}+a_{1}^{t+s}y_{0}
\end{eqnarray*}\]

<table>
  <tbody>
    <tr>
      <td>If $$</td>
      <td>a_{1}</td>
      <td>&lt;1\(, as\)t\rightarrow \infty$$</td>
    </tr>
  </tbody>
</table>

\[\begin{eqnarray*}
\lim_{t\rightarrow \infty}y_{t}&amp;=&amp;\frac{a_{0}}{1-a_{1}}+\sum_{i=0}^{\infty}a_{1}^{i}\varepsilon_{t-i}
\end{eqnarray*}\]

\[\begin{eqnarray*}
Var(y_{t})&amp;=&amp;E\left[ (y_{t}-\mu)^{2}\right]=E\left[ (\varepsilon_{t}+a_{1}\varepsilon_{t-1}+a_{1}^{2}\varepsilon_{t-2}+\cdots)^{2}\right]\\
&amp;=&amp;\sigma^{2}\left[ (1+a_{1}^{2}+a_{1}^{4}+\cdots)\right]=\frac{\sigma^{2}}{1-a_{1}^{2}}
\end{eqnarray*}\]

\[\begin{eqnarray*}
Cov(y_{t}, y_{t-s})&amp;=&amp;E\left[ (y_{t}-\mu)(y_{t-s}-\mu)\right]\\
&amp;=&amp;E\left[ (\varepsilon_{t}+a_{1}\varepsilon_{t-1}+a_{1}^{2}\varepsilon_{t-2}+\cdots)(\varepsilon_{t-s}+a_{1}\varepsilon_{t-s-1}+a_{1}^{2}\varepsilon_{t-s-2}+\cdots)\right]\\
&amp;=&amp;E\left( a_{1}^{s}\varepsilon_{t-s}^{2}+a_{1}^{s+2}\varepsilon_{t-s-1}^{2}+a_{1}^{s+4}\varepsilon_{t-s-2}^{2}+\cdots\right)\\
&amp;=&amp;\sigma^{2}a_{1}^{s}(1+a_{1}^{2}+a_{1}^{4}+\cdots)\\
&amp;=&amp;\frac{\sigma^{2}a_{1}^{s}}{1-a_{1}^{2}}
\end{eqnarray*}\]

<h2 id="the-autocorrelation-function-of-an-ar2-process">The Autocorrelation Function of an AR(2) Process</h2>
<p>We assume that \(a_{0}=0\), which implies that \(E(y_{t})=0\). Adding or subtracting any constant from a variable does not change its variance, covariance, correlation coefficient, etc.</p>

<p>Using Yule-Walker equations: multiply the second-order D.E by \(y_{t-s}\) for s\(=0, 1, 2, \cdots\), and take expectations</p>

\[\begin{eqnarray*}
Ey_{t}y_{t}&amp;=&amp;a_{1}Ey_{t-1}y_{t}+a_{2}Ey_{t-2}y_{t}+E\varepsilon_{t}y_{t}\\
Ey_{t}y_{t-1}&amp;=&amp;a_{1}Ey_{t-1}y_{t-1}+a_{2}Ey_{t-2}y_{t-1}+E\varepsilon_{t}y_{t-1}\\
Ey_{t}y_{t-2}&amp;=&amp;a_{1}Ey_{t-1}y_{t-2}+a_{2}Ey_{t-2}y_{t-2}+E\varepsilon_{t}y_{t-2}\\
&amp;\vdots&amp;\\
Ey_{t}y_{t-s}&amp;=&amp;a_{1}Ey_{t-1}y_{t-s}+a_{2}Ey_{t-2}y_{t-s}+E\varepsilon_{t}y_{t-s}\\
\end{eqnarray*}\]

<p>By definition, the autocovariances of a stationary series are such</p>

\[\begin{eqnarray*}
Ey_{t}y_{t-s}&amp;=&amp;Ey_{t-s}y_{t}=Ey_{t-k}y_{t-k-s}=\gamma_{s}
\end{eqnarray*}\]

<p>We also know that coefficient on \(\varepsilon_{t}\) is unity so that \(E\varepsilon_{t}y_{t}=\sigma^{2}\), and \(E\varepsilon_{t}y_{t-s}=0\), so</p>

\[\begin{eqnarray*}
\gamma_{0}&amp;=&amp;a_{1}\gamma_{1}+a_{2}\gamma_{2}+\sigma^{2}\\
\gamma_{1}&amp;=&amp;a_{1}\gamma_{0}+a_{2}\gamma_{1}\\
\gamma_{2}&amp;=&amp;a_{1}\gamma_{1}+a_{2}\gamma_{0}\\
&amp;\vdots&amp;\\
\gamma_{s}&amp;=&amp;a_{1}\gamma_{s-1}+a_{2}\gamma_{s-2}
\end{eqnarray*}\]

<p>Now we can get the ACF</p>

\[\begin{eqnarray*}
\rho_{1}&amp;=&amp;a_{1}\rho_{0}+a_{2}\rho_{1}\\
\rho_{s}&amp;=&amp;a_{1}\rho_{s-1}+a_{2}\rho_{s-2}
\end{eqnarray*}\]

<p>We know \(\rho_{0}=1\), so</p>

\[\begin{eqnarray*}
\rho_{1}&amp;=&amp;a_{1}+a_{2}\rho_{1}\\
\rho_{1}&amp;=&amp;\frac{a_{1}}{1-a_{2}}
\end{eqnarray*}\]

<h2 id="the-autocorrelation-function-of-an-ma1-process">The Autocorrelation Function of an MA(1) Process</h2>
<p>Consider the MA(1) process \(y_{t}=\varepsilon_{t}+\beta\varepsilon_{t-1}\)</p>

<p>Applying the Yule-Walker equations</p>

\[\begin{eqnarray*}
\gamma_{0}&amp;=&amp;E(y_{t}y_{t})=E\left[ (\varepsilon_{t}+\beta\varepsilon_{t-1})(\varepsilon_{t}+\beta\varepsilon_{t-1})\right]=(1+\beta^{2})\sigma^{2}\\
\gamma_{1}&amp;=&amp;E(y_{t}y_{t-1})=E\left[ (\varepsilon_{t}+\beta\varepsilon_{t-1})(\varepsilon_{t-1}+\beta\varepsilon_{t-2})\right]=\beta^{2}\sigma^{2}\\
\gamma_{2}&amp;=&amp;E(y_{t}y_{t-2})=E\left[ (\varepsilon_{t}+\beta\varepsilon_{t-1})(\varepsilon_{t-2}+\beta\varepsilon_{t-3})\right]=0\\
\gamma_{s}&amp;=&amp;E(y_{t}y_{t-s})=E\left[ (\varepsilon_{t}+\beta\varepsilon_{t-1})(\varepsilon_{t-s}+\beta\varepsilon_{t-s-1})\right]=0 \ \ \forall t&gt;2
\end{eqnarray*}\]

<p>So the ACF of MA(1)</p>

\[\begin{eqnarray*}
\rho_{0}&amp;=&amp;1\\
\rho_{1}&amp;=&amp;\frac{\gamma_{1}}{\gamma_{0}}=\frac{\beta^{2}}{1+\beta^{2}}\\
\rho_{s}&amp;=&amp;0 \ \ \forall t&gt;1
\end{eqnarray*}\]

<p>The Autocorrelation Function of an ARMA(1,1) Process
Consider the ARMA(1,1) \(y_{t}=a_{1}y_{t-1}+\varepsilon_{t}+\beta_{1}\varepsilon_{t-1}\)</p>

\[\begin{eqnarray*}
Ey_{t}y_{t}&amp;=&amp;a_{1}Ey_{t-1}y_{t}+E\varepsilon_{t}y_{t}+\beta_{1}E\varepsilon_{t-1}y_{t} \Rightarrow \\
\gamma_{0}&amp;=&amp;a_{1}\gamma_{1}+\sigma^{2}+\beta_{1}(a_{1}+\beta_{1})\sigma^{2}\\
Ey_{t}y_{t-1}&amp;=&amp;a_{1}Ey_{t-1}y_{t-1}+E\varepsilon_{t}y_{t-1}+\beta_{1}E\varepsilon_{t-1}y_{t-1} \Rightarrow \\
\gamma_{1}&amp;=&amp;a_{1}\gamma_{0}+\beta_{1}\sigma^{2}\\
Ey_{t}y_{t-2}&amp;=&amp;a_{1}Ey_{t-1}y_{t-2}+E\varepsilon_{t}y_{t-2}+\beta_{1}E\varepsilon_{t-1}y_{t-2} \Rightarrow \\
\gamma_{2}&amp;=&amp;a_{1}\gamma_{1}\\
Ey_{t}y_{t-s}&amp;=&amp;a_{1}Ey_{t-1}y_{t-s}+E\varepsilon_{t}y_{t-s}+\beta_{1}E\varepsilon_{t-1}y_{t-s} \Rightarrow \\
\gamma_{s}&amp;=&amp;a_{1}\gamma_{s-1}
\end{eqnarray*}\]

<p>Solve the equations and get</p>

\[\begin{eqnarray*}
\gamma_{0}&amp;=&amp;\frac{1+\beta_{1}^{2}+2a_{1}\beta_{1}}{1-a_{1}^{2}}\sigma^{2}\\
\gamma_{1}&amp;=&amp;\frac{(1+a_{1}\beta_{1})(a_{1}+\beta_{1})}{1-a_{1}^{2}}\sigma^{2}
\end{eqnarray*}\]

<p>And the AFC</p>

\[\begin{eqnarray*}
\rho_{0}&amp;=&amp;1\\
\rho_{1}&amp;=&amp;\frac{\gamma_{1}}{\gamma_{0}}=\frac{(1+a_{1}\beta_{1})(a_{1}+\beta_{1})}{1+\beta_{1}^{2}+2a_{1}\beta_{1}}\\
\rho_{s}&amp;=&amp;a_{1}\rho_{s} \ \ \forall t&gt;1
\end{eqnarray*}\]

<h1 id="the-partial-autoorrelation-function">The Partial Autoorrelation Function</h1>
<p>In AR(1) process, \(y_{t}\) and \(y_{t-2}\) are correlated even though \(y_{t-2}\) does not directly appear in the model. \(\rho_{2}=corr(y_{t}, y_{t-1})\times corr(y_{t-1}, y_{t-2})=\rho_{1}^{2}\). All such “indirect” correlations are present in the ACF. In contrast, the partial autocorrelation between \(y_{t}\) and \(y_{t-s}\) climinates the effects of the intervening values \(y_{t-1}\) through \(y_{t-s+1}\).</p>

<p>Method to find the PACF:</p>

<ul>
  <li>Form the series \(\{y_{t}^{\ast}\}\), where \(y_{t}^{\ast}\equiv y_{t}-\mu\)</li>
  <li>Form the first-order autoregression equation:</li>
</ul>

\[\begin{eqnarray*}
	y_{t}^{\ast}&amp;=&amp;\phi_{11}y_{t-1}^{\ast}+e_{i}\\
	y_{t}^{\ast}&amp;=&amp;\phi_{21}y_{t-1}^{\ast}+\phi_{22}y_{t-2}^{\ast}+e_{i}
	\end{eqnarray*}\]

<p>where \(\phi_{11}\) is the partial autocorrelation between \(y_{t}\) and \(y_{t-1}\), \(\phi_{22}\) is the partial autocorrelation between \(y_{t}\) and \(y_{t-2}\). Repeating this process for all additional lags s yields the PACF.</p>

\[\begin{array}{lll}\hline
\text{Process}		&amp;		ACF		&amp; PACF\\ \hline
\text{White-noise}&amp; 	\rho_{s}=0 &amp; \phi_{ss}=0\\
AR(1): a_{1}&gt;0 &amp;\rho_{s}=a_{1}^{s} 	&amp;\phi_{11}=\rho_{1}; \phi_{ss}=0 for s\geq2\\
AR(1): a_{1}&lt;0 &amp;\rho_{s}=a_{1}^{s} 	&amp;\phi_{11}=\rho_{1}; \phi_{ss}=0 for s\geq2\\
AR(p)&amp; \text{Decays toward zero.}  	&amp; \text{Spikes through lag p} \\
	&amp; \text{Coefficients may oscillate} &amp; \phi_{ss}=0 for s\geq p\\
MA(1): \beta&gt;0 &amp; \rho_{s}=0, \text{for} s\geq 2 &amp; \text{Oscillating decay:} \phi_{11}&gt;0\\
MA(1): \beta&gt;0 &amp; \rho_{s}=0, \text{for} s\geq 2 &amp; \text{Decay:} \phi_{11}&lt;0 \\
ARMA(1, 1): a_{1}&gt;0 &amp; 	&amp;\\
ARMA(1, 1): a_{1}&lt;0 &amp; 	&amp;\\
ARMA(p, q)&amp; 	&amp;\\ \hline
\end{array}\]

<h1 id="sample-autoorrelations">Sample Autoorrelations</h1>
<p>Given that a series is stationary, we can use the sample mean, variance and autocorrelations to estimate the parameters of the actual data-generating process.</p>

<p>The estimates of \(\mu\), \(\sigma^{2}\) and \(\rho\):</p>

\[\begin{eqnarray*}
\overline{y}&amp;=&amp;\frac{\sum_{t=1}^{T}y_{t}}{T}\\
\hat{\sigma^{2}}&amp;=&amp;\frac{\sum_{t=1}^{T}(y_{t}-\overline{y})^{2}}{T}\\
r_{s}&amp;=&amp;\frac{\sum_{t=s+1}^{T}(y_{t}-\overline{y})(y_{t-s}-\overline{y})}{\sum_{t=1}^{T}(y_{t}-\overline{y})^{2}}
\end{eqnarray*}\]

<p>Box and Pierce used the sample autocorrelations to form the statistic</p>

\[\begin{eqnarray*}
Q&amp;=&amp;T\sum_{k=1}^{s}r_{k}^{2}
\end{eqnarray*}\]

<p>If the data are generated from a stationary ARMA process, Q is asymptotically \(\chi^{2}(s)\) distribution.</p>

<p>Ljung and Box test</p>

\[\begin{eqnarray*}
Q&amp;=&amp;T(T+2)\sum_{k=1}^{s}\frac{r_{k}^{2}}{T-k} \sim \chi^{2}(s)
\end{eqnarray*}\]

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Lingbo  Shen. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>
  </body>

  <!-- jQuery -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  <!-- Mansory & imagesLoaded -->
  <script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  <script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
  
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  
</html>

