<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lingbo  Shen | Davidson and MacKinnon Chapter 7&mdash;Generalized Least Squares and Related Topics</title>
    <meta name="author" content="Lingbo  Shen" />
    <meta name="description" content="The seventh chapter of Econometric Theory and Methods" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />

    <meta name="google-site-verification" content="45VVBTzbWI9CPrT2UtIdTtL6imAnvMgtV1VttR2zNGI" />
    
    <!-- Bootstrap & MDB -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://lingboshen.github.io/blog/2022/DMCh7/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://lingboshen.github.io/"><span class="font-weight-bold">Lingbo</span>   Shen</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">Home</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Research</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>
<!-- Chinese -->
              <li class="nav-item ">
                <a class="nav-link" href="/chinese/">Chinese/中文</a>
              </li>

              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Davidson and MacKinnon Chapter 7—Generalized Least Squares and Related Topics</h1>
    <p class="post-meta">October 23, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/note">
          <i class="fas fa-hashtag fa-sm"></i> note</a>  
          <a href="/blog/tag/econometics">
          <i class="fas fa-hashtag fa-sm"></i> econometics</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h1 id="introduction">Introduction</h1>
<p>We will be concerned with the model</p>

\[\begin{eqnarray}
\label{eq7.1}
y&amp;=&amp;X\beta+u\ \ ,\ \ E(uu^{T})=\Omega
\end{eqnarray}\]

<p>where  \(\Omega\) is the covariance matrix of the error terms, is a positive definite \(n\times n\) matrix. We consider following cases for \(\Omega\):</p>

<ul>
  <li>\(\Omega=\sigma^{2}I\): (\ref{eq7.1}) is the classic linear regression model</li>
  <li>\(\Omega\) is diagonal with nonconstant diagonal elements: heteroskedastic</li>
  <li>\(\Omega\) is not diagonal</li>
</ul>

<h1 id="the-gls-estimator">The GLS Estimator</h1>
<p>We transform the model (\ref{eq7.1}) so that the transformed model satisfied the conditions of the Gauss-Markov theorem. Let \(\Psi\) be a \(n\times n\) triangular matrix that satisfies</p>

\[\begin{eqnarray}
\label{eq7.2}
\Omega^{-1}&amp;=&amp;\Psi\Psi^{T}
\end{eqnarray}\]

<p>Premultiplying (\ref{eq7.1}) by \(\Psi^{T}\) gives</p>

\[\begin{eqnarray}
\label{eq7.3}
\Psi^{T}y&amp;=&amp;\Psi^{T}X\beta+\Psi^{T}u
\end{eqnarray}\]

<p>The OLS estimator of \(\beta\) from regression (\ref{eq7.3}) is</p>

\[\begin{eqnarray}
\label{eq7.4}
\hat{\beta}_{GLS}&amp;=&amp;\left(X^{T}\Psi\Psi^{T}X\right)^{-1}X^{T}\Psi\Psi^{T}y=\left(X^{T}\Omega^{-1}X\right)^{-1}X^{T}\Omega^{-1}y
\end{eqnarray}\]

<p>This estimator is called the <strong>generalized least squares</strong>, or <strong>GLS</strong>, estimator of \(\beta\)</p>

<p>It is easy to show that the covariance matrix of transformed model (\ref{eq7.3}) is the identity matrix</p>

\[\begin{eqnarray*}
E(\Psi^{T}uu^{T}\Psi)&amp;=&amp;\Psi^{T}E(uu^{T})\Psi\\
&amp;=&amp;\Psi^{T}\Omega\Psi\\
&amp;=&amp;\Psi^{T}(\Psi\Psi^{T})^{-1}\Psi\\
&amp;=&amp;\Psi^{T}(\Psi^{T})^{-1}\Psi^{-1}\Psi=I\\
\end{eqnarray*}\]

<p>And we find</p>

\[\begin{eqnarray}
\label{eq7.5}
Var(\hat{\beta}_{GLS})&amp;=&amp;(X^{T}\Psi\Psi^{T}X)^{-1}=(X^{T}\Omega^{-1}X)^{-1}
\end{eqnarray}\]

<p>The generalized least squares estimator \(\hat{\beta}_{GLS}\) can also be obtained by minimizing the <strong>GLS criterion function</strong></p>

\[\begin{eqnarray}
\nonumber
(\Psi^{T}u)^{T}\Psi^{T}u&amp;=&amp;(\Psi^{T}y-\Psi^{T}X\beta)^{T}(\Psi^{T}y-\Psi^{T}X\beta)\\
\nonumber
&amp;=&amp;(y-X\beta)^{T}\Psi\Psi^{T}(y-X\beta)\\
\label{eq7.6}
&amp;=&amp;(y-X\beta)^{T}\Omega^{-1}(y-X\beta)
\end{eqnarray}\]

<h2 id="efficiency-of-the-gls-estimator">Efficiency of the GLS Estimator</h2>
<p>The GLS estimator \(\hat{\beta}_{GLS}\) defined in (\ref{eq7.4}) is also the solution of the set of moment conditions</p>

\[\begin{eqnarray}
\nonumber
(\Psi^{T}X)^{T}(\Psi^{T}y-\Psi^{T}X\beta)&amp;=&amp;0\\
\nonumber
X^{T}\Psi\Psi^{T}(y-X\beta)&amp;=&amp;0\\
\label{eq7.7}
X^{T}\Omega^{-1}(y-X\beta_{GLS})&amp;=&amp;0
\end{eqnarray}\]

<p>GLS estimator is a method of moments estimator. A general MM estimator for the linear regression model (\ref{eq7.1}) is defined in terms of an \(n\times k\) matrix of exogenous variables \(W\)</p>

\[\begin{eqnarray}
\label{eq7.8}
W^{T}(y-X\beta)&amp;=&amp;0
\end{eqnarray}\]

<p>We obtain the MM estimator</p>

\[\begin{eqnarray}
\label{eq7.9}
\hat{\beta}_{W}&amp;\equiv&amp;(W^{T}X)^{-1}W^{T}y
\end{eqnarray}\]

<p>The GLS estimator (\ref{eq7.4}) is a special case of the MM estimator, with \(W=\Omega^{-1}X\).</p>

<p>We can rewrite \(\hat{\beta}_{W}=\beta_{0}+(W^{T}X)^{-1}W^{T}u\). Thus, the covariance matrix of \(\hat{\beta}_{W}\) is</p>

\[\begin{eqnarray}
\nonumber
Var(\hat{\beta}_{W})&amp;=&amp;E\left((\hat{\beta}_{W}-\beta_{0})(\hat{\beta}_{W}-\beta_{0})^{T}\right)\\
\nonumber
&amp;=&amp;E\left((W^{T}X)^{-1}W^{T}uu^{T}W(X^{T}W)^{-1}\right)\\
\label{eq7.10}
&amp;=&amp;(W^{T}X)^{-1}W^{T}\Omega W(X^{T}W)^{-1}
\end{eqnarray}\]

<p>The efficiency of the GLS estimator can be verified by showing that the difference between (\ref{eq7.10}), the covariance matrix of the MM estimator \(\hat{\beta}_{W}\), and (\ref{eq7.5}), the covariance matrix of GLS estimator, \(\hat{\beta}_{GLS}\), is a positive semidefinite matrix. That is, the matrix</p>

\[\begin{eqnarray}
\label{eq7.11}
(W^{T}X)^{-1}W^{T}\Omega W(X^{T}W)^{-1}
&amp;-&amp;(X^{T}\Omega^{-1}X)^{-1}
\end{eqnarray}\]

<p>is a positive semidefinite matrix.</p>

<p>By the theory of linear algebra, (\ref{eq7.11}) is positive semidefinite if and only if</p>

\[\begin{eqnarray*}
\left((X^{T}\Omega^{-1}X)^{-1}\right)^{-1}&amp;-&amp;\left((W^{T}X)^{-1}W^{T}\Omega W(X^{T}W)^{-1}\right)^{-1}\\
X^{T}\Omega^{-1}X&amp;-&amp;X^{T}W(W^{T}\Omega W)^{-1}W^{T}X\\
\end{eqnarray*}\]

<p>is positive semidefinite.</p>

<p>The GLS estimator \(\hat{\beta}_{GLS}\) is typically more efficient than the more general MM estimator \(\hat{\beta}_{W}\). Because the OLS estimator \(\hat{\beta}_{OLS}\) is just special case of \(\hat{\beta}_{W}\) when \(W=X\), we could conclude that \(\hat{\beta}_{GLS}\) will in most cases be more efficient, and will never be less efficient, than the OLS estimator \(\hat{\beta}_{OLS}\).</p>

<h1 id="computing-gls-estimates">Computing GLS Estimates</h1>
<p>Suppose \(\Omega=\sigma^{2}\Delta\), and \(\Delta\) is known and \(\sigma^{2}\) is unknown. If we replace \(\Omega\) by \(\Delta\) in (\ref{eq7.2}) of \(\Psi\),</p>

\[\begin{eqnarray*}
\Delta^{-1}&amp;=&amp;\Psi\Psi^{T}
\end{eqnarray*}\]

<p>we still run regression (\ref{eq7.3})</p>

\[\begin{eqnarray*}
\Psi^{T}y&amp;=&amp;\Psi^{T}X\beta+\Psi^{T}u
\end{eqnarray*}\]

<p>The GLS estimates will be the same whether we use \(\Omega\) or \(\Delta\).</p>

<p>However, if \(\sigma^{2}\) is known, we can use the true covariance matrix (\ref{eq7.5}). Otherwise, we must estimate covariance matrix</p>

\[\begin{eqnarray}
\label{eq7.12}
\widehat{Var}(\hat{\beta}_{GLS})&amp;=&amp;s^{2}(X^{T}\Delta^{-1}X)^{-1}
\end{eqnarray}\]

<h2 id="weighted-least-squares">Weighted Least Squares</h2>
<p>GLS estimation will be easy to do if the matrix \(\Psi\) is known and allow us to calculate \(\Psi^{T}x\).</p>

<p>When error terms are heteroskedastic but uncorrelated, i.e., \(\Omega\) is diagonal. Let \(\omega_{t}^{2}\) denote the \(t^{th}\) diagonal element of \(\Omega\), and \(\omega_{t}^{-2}\) is the \(t^{th}\) diagonal element of \(\Omega^{-1}\).  \(\Psi\) can be chosen as the diagonal matrix with \(t^{th}\) diagonal element \(\omega_{t}^{-1}\)</p>

\[\begin{eqnarray*}
\Omega=\left[\begin{array}{cccc}
\omega_{1}^{2}&amp;0&amp;\cdots&amp;0\\
0&amp;\omega_{2}^{2}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\
0&amp;0&amp;\cdots&amp;\omega_{k}^{2}
\end{array}
\right]&amp;,&amp;\Omega^{-1}=\left[\begin{array}{cccc}
\omega_{1}^{-2}&amp;0&amp;\cdots&amp;0\\
0&amp;\omega_{2}^{-2}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\
0&amp;0&amp;\cdots&amp;\omega_{k}^{-2}
\end{array}
\right]
\end{eqnarray*}\]

<p>and</p>

\[\begin{eqnarray*}
\Psi&amp;=&amp;\left[\begin{array}{cccc}
\omega_{1}^{-1}&amp;0&amp;\cdots&amp;0\\
0&amp;\omega_{2}^{-1}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots &amp;\vdots\\
0&amp;0&amp;\cdots&amp;\omega_{k}^{-1}
\end{array}
\right]
\end{eqnarray*}\]

<p>Thus we see that, for a typical observation, regression (\ref{eq7.3}) can be written as</p>

\[\begin{eqnarray}
\label{eq7.13}
\omega_{t}^{-1}y_{t}&amp;=&amp;\omega_{t}^{-1}X_{t}\beta+\omega_{t}^{-1}u_{t}
\end{eqnarray}\]

<p>the variance of the error term is clearly 1.</p>

<p>This special case of GLS estimation is often called <strong>weighted least squares</strong>, or <strong>GLS</strong>.</p>

<h1 id="feasible-generalized-least-squares">Feasible Generalized Least Squares</h1>
<p>In practice, the covariance matrix \(\Omega\) is often not known even up to a scalar factor. This makes it impossible to compute GLS estimates.</p>

<p>However, it is reasonable to suppose that \(\Omega\), or \(\Delta\), depends in a known way on a vector of unknown parameters \(\gamma\). If so, it may be possible to estimate \(\gamma\) consistently, so as to obtain \(\Omega(\hat{\gamma})\). Then \(\Psi(\hat{\gamma})\) can be defined in (\ref{eq7.2}), and GLS estimates computed conditional on \(\Psi(\hat{\gamma})\). This type of procedure is called <strong>feasible generalized least squares</strong>, or <strong>feasible GLS</strong>.</p>

<p>For example, suppose the feasible GLS estimates of the linear regression model is</p>

\[\begin{eqnarray}
\label{eq7.14}
y_{t}=X_{t}\beta+u_{t}&amp;,&amp;E(u_{t}^{2})=\exp(Z_{t}\gamma)
\end{eqnarray}\]

<p>where \(\beta\) and \(\gamma\) are a \(k\)-vector and an \(l\)-vector of unknown parameters, respectively. \(X_{t}\) and \(Z_{t}\) are comformably dimensioned row vectors of observation set on which we are conditioning. Some of the elements of \(Z_{t}\) may well belong to \(X_{t}\). The function \(\exp(Z_{t}\gamma)\) is an example of a <strong>skedastic function</strong>.</p>

<p>Steps to do FGLS</p>

<ul>
  <li>
    <p>To obtain consistent estimates of \(\gamma\), we must first obtain consistent estimates of the error terms in (\ref{eq7.14}).</p>
  </li>
  <li>
    <p>Computing OLS estimates \(\hat{\beta}\) allows us to calculate a vector of OLS residuals with typical element \(\hat{u}_{t}\).</p>
  </li>
  <li>
    <p>Run auxiliary linear regression</p>
  </li>
</ul>

\[\begin{eqnarray}
\label{eq7.15}
\log \hat{u}_{t}^{2}&amp;=&amp;Z_{t}\gamma+v_{t}
\end{eqnarray}\]

<p>to find OLS estimates \(\hat{\gamma}\).</p>

<ul>
  <li>Compute</li>
</ul>

\[\begin{eqnarray*}
\hat{\omega}_{t}&amp;=&amp;\left(\exp(Z_{t}\hat{\gamma})\right)^{1/2}
\end{eqnarray*}\]

<ul>
  <li>Feasible GLS of \(\beta\) are obtained by using ordinary least squares to estimate regression (\ref{eq7.13}), with the estimates \(\hat{\omega}_{t}\) replacing the unknown \(\omega_{t}\).</li>
</ul>

<h2 id="why-fgls-works">Why FGLS Works</h2>
<p>Under suitable regularity conditions, it can be shown that this type of procedure leads a feasible GLS estimator \(\hat{\beta}_{F}\) that is consistent and asymptotically equivalent to the GLS estimator \(\hat{\beta}_{GLS}\).</p>

<h1 id="heteroskedasticity">Heteroskedasticity</h1>
<p>If we have no information on the form of the skedastic function, it may be prudent to employ an HCCME, especially if the sample size is large.</p>

<p>If we have information on the form of the skedastic function, we might well wish to use weighted least squares.</p>

<p>It makes no difference asymptotically whether the \(\omega_{t}\) are known or merely estimated consistently, although it can certainly make a substantial difference in finite samples. Asymptotically, at least, the usual OLS covariance matrix is just as valid with feasible WLS as with WLS.</p>

<h2 id="testing-for-heteroskedasticity">Testing for Heteroskedasticity</h2>
<p>Before doing so, it is advisable to perform a <strong>specification test</strong> of the \underline{null hypothesis that the error terms are homoskedastic against whatever heteroskedastic} alternatives may seem reasonable.</p>

<p>White showed that, in a linear regression model, if \(E(u_{t}^{2})\) is constant conditional on the squares and cross-products of all the regressors, then there is no need to use an HCCME.</p>

<h1 id="autoregressive-processes">Autoregressive Processes</h1>
<p>The error terms for nearby observations may be correlated, or may appear to be correlated. This phenomenon is most commonly encountered in models estimated with time-series data, where it is known as <strong>serial correlation</strong> or <strong>autocorrelation</strong>.</p>

<p>If there is reason to believe that serial correlation may be present</p>

<ul>
  <li>Step 1: Test the null hypothesis that the errors are serially uncorrelated against a plausible alternative that involves serial correlation.</li>
  <li>Step 2: If evidence of serial correlation is found, estimate a model that accounts for it based on NLS and GLS.</li>
  <li>Step 3: Verify that the model which accounts for serial correlation is compatible with the data</li>
</ul>

<h2 id="the-ar1-process">The AR(1) Process</h2>
<p>One of the simplest and most commonly used stochastic processes is the <strong>first-order autoregressive process</strong>, or <strong>AR(1) process</strong>, which can be written as</p>

\[\begin{eqnarray}
\label{eq7.16}
u_{t}=\rho u_{t-1}+\varepsilon_{t}&amp;,&amp;\varepsilon_{t}\sim \mbox{IID}(0,\sigma_{\varepsilon})^{2},\ \ |\rho|&lt;1
\end{eqnarray}\]

<p>It is assumed that \(\varepsilon_{t}\) is independent of \(\varepsilon_{s}\) for all \(s\neq t\), \(\varepsilon_{t}\) is an innovation.</p>

<table>
  <tbody>
    <tr>
      <td>$$</td>
      <td>\rho</td>
      <td>&lt;1$$ is called a <strong>stationarity condition</strong>, because it is necessary for the AR(1) process to be <strong>stationary</strong>.</td>
    </tr>
  </tbody>
</table>

<p>The <strong>covariance stationarity/wide sense stationarity</strong> is defined as \(E(u_{t})\) and \(Var(u_{t}\) exist and are independent of \(t\) and if the covariance \(Cov(u_{t},u_{t-j})\) is independent of \(t\).</p>

<p>We can then compute the variance of \(u_{t}\) by substituting successively for \(u_{t-1}\), \(u_{t-2}\), \(u_{t-3}\), and so on in (\ref{eq7.16})</p>

\[\begin{eqnarray}
\label{eq7.17}
u_{t}&amp;=&amp;\varepsilon_{t}+\rho\varepsilon_{t-1}+\rho^{2}\varepsilon_{t-2}+\rho^{3}\varepsilon_{t-3}+\cdots
\end{eqnarray}\]

<p>The variance of \(u_{t}\) is seen to be</p>

\[\begin{eqnarray}
\label{eq7.18}
\sigma_{u}^{2}&amp;=&amp;\sigma_{\varepsilon}^{2}+\rho^{2}\sigma_{\varepsilon}^{2}+\rho^{4}\sigma_{\varepsilon}^{2}+\rho^{6}\sigma_{\varepsilon}^{2}+\cdots=\frac{\sigma_{\varepsilon}^{2}}{1-\rho^{2}}
\end{eqnarray}\]

<p>We can get that \(Var(u_{t})=\sigma_{\varepsilon}^{2}/(1-\rho^{2})\) for all \(t\).</p>

<p>The covariance for \(u_{t}\) and \(u_{t-1}\) of the AR(1) is</p>

\[\begin{eqnarray*}
Cov(u_{t},u_{t-1})&amp;=&amp;E(u_{t}u_{t-1})\\
&amp;=&amp;E((\rho u_{t-1}+\varepsilon_{t})u_{t-1})\\
&amp;=&amp;\rho \sigma_{u}^{2}
\end{eqnarray*}\]

<p>If the AR(1) process (\ref{eq7.16}) is stationary, the covariance matrix of the vector \(u\) can be written as</p>

\[\begin{eqnarray}
\label{eq7.19}
\Omega(\rho)&amp;=&amp;\frac{\sigma_{\varepsilon}^{2}}{1-\rho^{2}}\left[
\begin{array}{ccccc}
1&amp;\rho&amp;\rho^{2}&amp;\cdots&amp;\rho^{n-1}\\
\rho&amp;1&amp;\rho^{2}&amp;\cdots&amp;\rho^{n-2}\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
\rho^{n-1}&amp;\rho^{n-2}&amp;\rho^{n-3}&amp;\cdots&amp;1
\end{array}
\right]
\end{eqnarray}\]

<h2 id="testing-for-serial-correlation">Testing for Serial Correlation</h2>

<ul>
  <li>Run OLS and compute OLS residuals \(\hat{u}=y-X
\hat{\beta}_{OLS}\)</li>
  <li>Auxiliary regression of OLS residuals \(\hat{u}\) on lagged values</li>
</ul>

\[\begin{eqnarray*}
\hat{u}_{t}&amp;=&amp;\rho \hat{u}_{t-1}+\varepsilon_{t}
\end{eqnarray*}\]

<ul>
  <li>\(t\) test for \(\rho=0\)</li>
</ul>

<h2 id="fgls-for-autoregression-process">FGLS for Autoregression Process</h2>
<p>If the \(u_{t}\) follow a stationary AR(1) process, that is, if \(|\rho|&lt;1\) and \(Var(u_{t})=\sigma_{u}^{2}=\sigma_{\varepsilon}^{2}/(1-\rho^{2})\), then the covariance matrix of the entire vector \(\hat{u}\) is \(\Omega(\rho)\) defined in (\ref{eq7.19}).</p>

<h1 id="moving-average-processes">Moving Average Processes</h1>
<h2 id="the-ma1-process">The MA(1) Process</h2>

\[\begin{eqnarray}
\label{eq7.20}
u_{t}=\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1}&amp;,&amp;\varepsilon_{t}\sim \mbox{IID}(0,\sigma_{\varepsilon}^{2})
\end{eqnarray}\]

<p>\(u_{t}\) is a weighted average of two successive innovations, \(\varepsilon_{t}\) and \(\varepsilon_{t-1}\).</p>

<p>The covariance matrix for an MA(1) process:</p>

\[\begin{eqnarray*}
Var(u_{t})&amp;=&amp;E\left((\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1})^{2}\right)=\sigma_{\varepsilon}^{2}+\alpha_{1}^{2}\sigma_{\varepsilon}^{2}=(1+\alpha_{1}^{2})\sigma_{\varepsilon}^{2}\\
Cov(u_{t},u_{t-1})&amp;=&amp;E\left((\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1})(\varepsilon_{t-1}+\alpha_{1}\varepsilon_{t-2})\right)=\alpha_{1}\sigma_{\varepsilon}^{2}\\
Cov(u_{t},u_{t-j})&amp;=&amp;E\left((\varepsilon_{t}+\alpha_{1}\varepsilon_{t-1})(\varepsilon_{t-j}+\alpha_{1}\varepsilon_{t-j-1})\right)=0
\end{eqnarray*}\]

<p>Therefore, the covariance matrix of the entire vector \(\hat{u}\) is</p>

\[\begin{eqnarray}
\label{eq7.21}
\sigma_{\varepsilon}\Delta(\alpha_{1})&amp;=&amp;\sigma_{\varepsilon}^{2}\left[\begin{array}{cccccc}
1+\alpha^{2}		&amp;\alpha	&amp;0	&amp;\cdots	&amp;0	&amp;0	\\
\alpha	&amp;1+\alpha^{2}		&amp;\alpha	&amp;\cdots	&amp;0	&amp;0\\
\vdots	&amp;\vdots	&amp;\vdots	&amp;	&amp;\vdots	&amp;\vdots\\
0	&amp;0	&amp;0	&amp;\cdots&amp;\alpha	&amp;1+\alpha^{2}
\end{array}\right]
\end{eqnarray}\]

<h1 id="models-for-panel-data">Models for Panel Data</h1>
<p>We restrict our attention to the linear regression model</p>

\[\begin{eqnarray}
\label{eq7.10.1}
y_{it}&amp;=&amp;X_{it}\beta+u_{it}
\end{eqnarray}\]

<p>where \(i=1,\dots, m\), \(t=1,\dots, T\), and \(X_{it}\) is a \(1\times k\) vector.</p>

<p>If certain shocks affect the same cross-sectional unit at all points in time, the error terms \(u_{it}\) and \(u_{is}\) will be correlated for all \(t \neq s\). Similarly, if certain shocks affect all cross-sectional units at the same point in time, the error terms \(u_{it}\) and \(u_{j}\)t will be correlated for all \(i \neq j\).</p>

<h2 id="error-components-models">Error-Components Models</h2>
<p>The idea is to specify the error term \(u_{it}\) in (\ref{eq7.10.1}) as consisting of two or three separate shocks, each of which is assumed to be independent of the others.</p>

\[\begin{eqnarray}
\label{eq7.10.2}
u_{it}&amp;=&amp;e_{t}+v_{i}+\varepsilon_{it}
\end{eqnarray}\]

<p>It is generally assumed that the \(e_t\) are independent across \(t\), the \(v_i\) are independent across \(i\), and the \(\varepsilon_{it}\), it are independent across all \(i\) and \(t\).</p>

<p>If the \(e_t\) and \(v_i\) are thought of as <strong>fixed effects</strong>, then they are treated as parameters to be estimated.</p>

<p>If they are thought of as <strong>random effects</strong>, then we must figure out the covariance matrix of the \(u_{it}\) as functions of the variances of the \(e_{t}\), \(v_{i}\), and \(\varepsilon_{it}\), and use feasible GLS.</p>

<h2 id="random-effects-estimation">Random-Effects Estimation</h2>
<p>Random-effects estimation requires that the \(v_{i}\) should be independent of \(X\). Then we have</p>

\[\begin{eqnarray}
\label{eq7.10.3}
E(u_{it}|X)&amp;=&amp;E(v_{i}+\varepsilon_{it}|X)=0
\end{eqnarray}\]

<p>However \(u_{it}\) are not IID. Assuming that \(v_{i}\sim IID(0, \sigma^{2}_{v})\), \(e_{t}\), and shocks are independent, we find that</p>

\[\begin{eqnarray*}
Var(u_{it})&amp;=&amp;\sigma_{v}^{2}+\sigma_{\varepsilon}^{2}\\
Cov(u_{it}, u_{is})&amp;=&amp;\sigma_{v}^{2}\\
Cov(u_{it}, u_{js})&amp;=&amp;0
\end{eqnarray*}\]

<p>These define the elements of the covariance matrix \(\Omega\)</p>

\[\begin{eqnarray*}
\Omega&amp;=&amp;\left[\begin{array}{cccc}
\Sigma&amp;0&amp;\cdots&amp;0\\
0&amp;\Sigma&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;&amp;\vdots\\
0&amp;0&amp;\cdots&amp;\Sigma\\
\end{array}\right]
\end{eqnarray*}\]

<p>where</p>

\[\begin{eqnarray}
\label{eq7.10.4}
\Sigma&amp;=&amp;\sigma_{\varepsilon}^{2}I_{T}+\sigma_{v}^{2}\iota\iota^{T}
\end{eqnarray}\]

<h2 id="properties-of-ols-estimator-in-random-effects-model">Properties of OLS estimator in Random Effects Model</h2>
<p>Under the assumption that \(E(u|X)=0\), the OLS estimator in random effects model is unbiased and consistent. However, since the error term is heteroskedastic, OLS estimator is not efficient.</p>


  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Lingbo  Shen. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>
  </body>

  <!-- jQuery -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  <!-- Mansory & imagesLoaded -->
  <script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  <script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
  
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  
</html>

